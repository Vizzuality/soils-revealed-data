{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `GeoTIFFs` in Google Cloud Storage to `Zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import zarr\n",
    "import rioxarray as rxr\n",
    "import gcsfs\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023.5.0\n"
     ]
    }
   ],
   "source": [
    "print(xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path \n",
    "env_path = Path('.') / '.env'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download_blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, blob_name, file_name):\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "        \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    blob.download_to_filename(file_name)\n",
    "    \n",
    "    print(\n",
    "        \"File {} downloaded to {}.\".format(\n",
    "            blob_name, file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upload_blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, blob_name, file_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            file_name, blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upload_blob_s3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mboto3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msession\u001b[39;00m \u001b[39mimport\u001b[39;00m Session \u001b[39mas\u001b[39;00m boto3_session\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "from boto3.session import Session as boto3_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mboto3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msession\u001b[39;00m \u001b[39mimport\u001b[39;00m Session \u001b[39mas\u001b[39;00m boto3_session\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupload_blob_s3\u001b[39m(bucket_name, source_file_name, destination_blob_name):\n\u001b[1;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uploads a file to the bucket.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "from boto3.session import Session as boto3_session\n",
    "\n",
    "def upload_blob_s3(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    session = boto3_session(aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'), aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))\n",
    "    client = session.client(\"s3\")\n",
    "    client.upload_file(source_file_name, bucket_name, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3_session(aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'), aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))\n",
    "client = session.client(\"s3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**change_compression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_compression(input_tiff, output_tiff, compression='LZW'):\n",
    "    translateoptions = gdal.TranslateOptions(gdal.ParseCommandLine(f\"-of Gtiff -co COMPRESS={compression}\"))\n",
    "    gdal.Translate(output_tiff, input_tiff, options=translateoptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "scenarios = ['crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dYs = ['05', '10', '15', '20']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for dY in dYs:\n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY}.tif'\n",
    "        file_name_out = f'scenario_{scenario}_SOC_Y{dY}_new.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Check compression\n",
    "        dataset = gdal.OpenEx(f'../data/{file_name}')\n",
    "        md = dataset.GetMetadata('IMAGE_STRUCTURE')\n",
    "        \n",
    "        # Use dict.get method in case the metadata dict does not have a 'COMPRESSION' key\n",
    "        compression = md.get('COMPRESSION', None)\n",
    "        \n",
    "        if compression == 'ZSTD':\n",
    "            print(compression)\n",
    "            change_compression(f'../data/{file_name}', f'../data/{file_name_out}', compression='LZW')    \n",
    "            \n",
    "            os.rename(f'../data/{file_name_out}', f'../data/{file_name}')\n",
    "            # Upload tiff\n",
    "            upload_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        ## Remove tiff\n",
    "        os.remove(f'../data/{file_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From `GeoTIFFs` to `Zarr`\n",
    "\n",
    "We use the [xarray](http://xarray.pydata.org/en/stable/io.html#reading-and-writing-files) library to convert `GeoTIFFs` into `Zarr`. \n",
    "\n",
    "GeoTIFFs can be opened using [rasterio](http://xarray.pydata.org/en/stable/io.html#rasterio) with this xarray method: `xarray.open_rasterio`. Additionally, you can use [rioxarray](https://corteva.github.io/rioxarray/stable/) for reading GeoTiffs.\n",
    "\n",
    "To save `xarray.Datasets` as a `Zarr` we can us the [Xarrayâ€™s Zarr backend](http://xarray.pydata.org/en/stable/io.html#zarr). [Zarr](http://zarr.readthedocs.io/) is a Python package providing an implementation of chunked, compressed, N-dimensional arrays. Zarr has the ability to read and write xarray datasets directly from / to cloud storage buckets such as Amazon S3 and Google Cloud Storage.\n",
    "\n",
    "Xarray needs to read all of the zarr metadata when it opens a dataset. With version 2.3, Zarr will support a feature called consolidated metadata, which allows all metadata for the entire dataset to be stored with a single key (by default called `.zmetadata`). This can drastically speed up opening the store. To write consolidated metadata, pass the `consolidated=True` option to the `Dataset.to_zarr` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` on disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "vizz-data-transfer/SOC_maps/SOC_stock_EJSS\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/SOC_stock_EJSS/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `cstock030_YEAR_030cm_Q0.5.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock_EJSS/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_stock_EJSS/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = 'cstock030_' + year + '_030cm_Q0.5.tif' \n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "    \n",
    "    # replace all values equal to -32768 with np.nan\n",
    "    xda = xda.where(xda != -32768.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock_EJSS/'\n",
    "# Local path\n",
    "path = '../data/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_stock_EJSS/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = 'cstock030_' + year + '_030cm_Q0.5.tif' \n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # replace all values equal to -32768 with np.nan\n",
    "    xda = xda.where(xda != -32768.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm\n",
    "    \n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration2020/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_concentration2020/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOC_' + year + '_q0.5_D'+ dname + '.tif'\n",
    "        url = base_url + file_name\n",
    "         \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration2020/'\n",
    "# Local path\n",
    "path = '../data/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_concentration2020/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOC_' + year + '_q0.5_D'+ dname + '.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    # Save zarr \n",
    "    if i == 0:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save zarr \n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historic SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/SOC_maps/Historic/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOCS_DEPTH_year_YEAR_10km.tif`:\n",
    "- DEPTH: 0_30cm, 0_100cm, 0_200cm\n",
    "- YEAR: NoLU, 2010AD \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# Local path\n",
    "path = '../data/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Historic/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into local directory\n",
    "    if i == 0:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent_Nov/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_4326.tif`:\n",
    "- YEAR: 2000 - 2018 \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent_Nov/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent_Nov/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2000\n",
      "File SOC_maps/Recent_Nov/SOC_2000_4326.tif downloaded to ../../data/SOC_2000_4326.tif.\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:24: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/backends/file_manager.py\", line 210, in _acquire_with_cache_info\n",
      "    file = self._cache[self._key]\n",
      "           ~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/backends/lru_cache.py\", line 56, in __getitem__\n",
      "    value = self._cache[key]\n",
      "            ~~~~~~~~~~~^^^^^\n",
      "KeyError: [<function open at 0x7f9174a060c0>, ('../data/SOC_2000_4326.tif',), 'r', (), '71391a29-1166-46f5-9998-6fc12558e16c']\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"rasterio/_base.pyx\", line 308, in rasterio._base.DatasetBase.__init__\n",
      "  File \"rasterio/_base.pyx\", line 219, in rasterio._base.open_dataset\n",
      "  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\n",
      "rasterio._err.CPLE_OpenFailedError: ../data/SOC_2000_4326.tif: No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/magics/execution.py\", line 1319, in time\n",
      "    exec(code, glob, local_ns)\n",
      "  File \"<timed exec>\", line 24, in <module>\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/backends/rasterio_.py\", line 261, in open_rasterio\n",
      "    riods = manager.acquire()\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/backends/file_manager.py\", line 192, in acquire\n",
      "    file, _ = self._acquire_with_cache_info(needs_lock)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/backends/file_manager.py\", line 216, in _acquire_with_cache_info\n",
      "    file = self._opener(*self._args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/rasterio/env.py\", line 451, in wrapper\n",
      "    return f(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/rasterio/__init__.py\", line 304, in open\n",
      "    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n",
      "rasterio.errors.RasterioIOError: ../data/SOC_2000_4326.tif: No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1049, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1003, in get_records\n",
      "    lines, first = inspect.getsourcelines(etb.tb_frame)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/inspect.py\", line 1252, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iker/anaconda3/envs/geopy11/lib/python3.11/inspect.py\", line 1081, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent_Nov/'\n",
    "# Local path\n",
    "path = '../../data/processed/raster_data/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent_Nov/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:27: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      " â””â”€â”€ recent\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Year: 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:27: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      " â””â”€â”€ recent\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "CPU times: user 1min 25s, sys: 6.66 s, total: 1min 32s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent_Nov/'\n",
    "# Local path\n",
    "path = '../../data/processed/raster_data/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent_Nov/'\n",
    "\n",
    "times = [times[0], times[-1]]\n",
    "years = [years[0], years[-1]]\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    ## Download tiff\n",
    "    #download_blob(bucket_name, file_path+file_name, f'../../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../../data/raw/raster_data/{file_name}', chunks={\"x\": 4943, \"y\": 953}).squeeze().drop(\"band\")\n",
    "    \n",
    "    ## Remove tiff\n",
    "    #os.remove(f'../../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/Future/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "Scenario file are named scenario_xxxx_yyyy_Yzz\n",
    "\n",
    "Where: \n",
    "  - xxxx = scenario names:\n",
    "    - crop.MG \n",
    "    - crop.MGI\n",
    "    - crop.I\n",
    "    - grass.part\n",
    "    - grass.full\n",
    "    - rewilding\n",
    "    - degradation.ForestToGrass\n",
    "    - degradation.ForestToCrop\n",
    "    - degradation.NoDeforestation\n",
    "  - yyyy = either dSOC (for change in SOC stocks) or SOC (for absolute SOC stocks)\n",
    "  - zz = years after change in land use or management (05, 10, 15, 20)\n",
    "\n",
    "All scenario rasters are in units of Mg C / ha to 30 cm depth\n",
    "\n",
    "**Scenarios**\n",
    "- crop_MG: Cropland, improved management only \n",
    "- crop_MGI: Cropland, improved management and inputs \n",
    "- crop_I: Cropland, improved inputs only \n",
    "- grass-part: Grassland, partial restoration \n",
    "- grass-full: Grassland, full restoration \n",
    "- rewilding: Rewilding \n",
    "- degradation-ForestToGrass: Degradation (includes deforestation to degraded grassland condition)\n",
    "- degradation-ForestToCrop: Degradation (includes deforestation to cropland)\n",
    "- degradation-NoDeforestation: Degradation (no deforestation)\n",
    "\n",
    "**Output data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2018\n",
      "File SOC_maps/Recent_Nov/SOC_2018_4326.tif downloaded to ../data/SOC_2018_4326.tif.\n",
      "Scenario: crop_I\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: crop_MG\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: crop_MGI\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: grass_part\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: grass_full\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: rewilding\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: degradation_ForestToGrass\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: degradation_ForestToCrop\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: degradation_NoDeforestation\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (1, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (1,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: crop_I\n",
      "File SOC_maps/Future/scenario_crop_I_SOC_Y05_nov.tif downloaded to ../data/scenario_crop_I_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/crop_I.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_I_SOC_Y10_nov.tif downloaded to ../data/scenario_crop_I_SOC_Y10_nov.tif.\n",
      "File SOC_maps/Future/scenario_crop_I_SOC_Y15_nov.tif downloaded to ../data/scenario_crop_I_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/crop_I.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_I_SOC_Y20_nov.tif downloaded to ../data/scenario_crop_I_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/crop_I.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: crop_MG\n",
      "File SOC_maps/Future/scenario_crop_MG_SOC_Y05_nov.tif downloaded to ../data/scenario_crop_MG_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/crop_MG.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_MG_SOC_Y10_nov.tif downloaded to ../data/scenario_crop_MG_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/crop_MG.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_MG_SOC_Y15_nov.tif downloaded to ../data/scenario_crop_MG_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/crop_MG.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_MG_SOC_Y20_nov.tif downloaded to ../data/scenario_crop_MG_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/crop_MG.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: crop_MGI\n",
      "File SOC_maps/Future/scenario_crop_MGI_SOC_Y05_nov.tif downloaded to ../data/scenario_crop_MGI_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/crop_MGI.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_MGI_SOC_Y10_nov.tif downloaded to ../data/scenario_crop_MGI_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/crop_MGI.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_MGI_SOC_Y15_nov.tif downloaded to ../data/scenario_crop_MGI_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/crop_MGI.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_crop_MGI_SOC_Y20_nov.tif downloaded to ../data/scenario_crop_MGI_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/crop_MGI.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: grass_part\n",
      "File SOC_maps/Future/scenario_grass_part_SOC_Y05_nov.tif downloaded to ../data/scenario_grass_part_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/grass_part.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_grass_part_SOC_Y10_nov.tif downloaded to ../data/scenario_grass_part_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/grass_part.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_grass_part_SOC_Y15_nov.tif downloaded to ../data/scenario_grass_part_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/grass_part.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_grass_part_SOC_Y20_nov.tif downloaded to ../data/scenario_grass_part_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/grass_part.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: grass_full\n",
      "File SOC_maps/Future/scenario_grass_full_SOC_Y05_nov.tif downloaded to ../data/scenario_grass_full_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/grass_full.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_grass_full_SOC_Y10_nov.tif downloaded to ../data/scenario_grass_full_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/grass_full.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_grass_full_SOC_Y15_nov.tif downloaded to ../data/scenario_grass_full_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/grass_full.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_grass_full_SOC_Y20_nov.tif downloaded to ../data/scenario_grass_full_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/grass_full.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: rewilding\n",
      "File SOC_maps/Future/scenario_rewilding_SOC_Y05_nov.tif downloaded to ../data/scenario_rewilding_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/rewilding.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_rewilding_SOC_Y10_nov.tif downloaded to ../data/scenario_rewilding_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/rewilding.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_rewilding_SOC_Y15_nov.tif downloaded to ../data/scenario_rewilding_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/rewilding.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_rewilding_SOC_Y20_nov.tif downloaded to ../data/scenario_rewilding_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/rewilding.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: degradation_ForestToGrass\n",
      "File SOC_maps/Future/scenario_degradation_ForestToGrass_SOC_Y05_nov.tif downloaded to ../data/scenario_degradation_ForestToGrass_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToGrass.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_ForestToGrass_SOC_Y10_nov.tif downloaded to ../data/scenario_degradation_ForestToGrass_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToGrass.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_ForestToGrass_SOC_Y15_nov.tif downloaded to ../data/scenario_degradation_ForestToGrass_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToGrass.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_ForestToGrass_SOC_Y20_nov.tif downloaded to ../data/scenario_degradation_ForestToGrass_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToGrass.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: degradation_ForestToCrop\n",
      "File SOC_maps/Future/scenario_degradation_ForestToCrop_SOC_Y05_nov.tif downloaded to ../data/scenario_degradation_ForestToCrop_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToCrop.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_ForestToCrop_SOC_Y10_nov.tif downloaded to ../data/scenario_degradation_ForestToCrop_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToCrop.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_ForestToCrop_SOC_Y15_nov.tif downloaded to ../data/scenario_degradation_ForestToCrop_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToCrop.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_ForestToCrop_SOC_Y20_nov.tif downloaded to ../data/scenario_degradation_ForestToCrop_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/degradation_ForestToCrop.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "Scenario: degradation_NoDeforestation\n",
      "File SOC_maps/Future/scenario_degradation_NoDeforestation_SOC_Y05_nov.tif downloaded to ../data/scenario_degradation_NoDeforestation_SOC_Y05_nov.tif.\n",
      "s3://soils-revealed/degradation_NoDeforestation.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (2, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (2,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_NoDeforestation_SOC_Y10_nov.tif downloaded to ../data/scenario_degradation_NoDeforestation_SOC_Y10_nov.tif.\n",
      "s3://soils-revealed/degradation_NoDeforestation.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (3, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (3,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_NoDeforestation_SOC_Y15_nov.tif downloaded to ../data/scenario_degradation_NoDeforestation_SOC_Y15_nov.tif.\n",
      "s3://soils-revealed/degradation_NoDeforestation.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (4, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (4,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n",
      "File SOC_maps/Future/scenario_degradation_NoDeforestation_SOC_Y20_nov.tif downloaded to ../data/scenario_degradation_NoDeforestation_SOC_Y20_nov.tif.\n",
      "s3://soils-revealed/degradation_NoDeforestation.zarr is consoldiated? True\n",
      "/\n",
      " â””â”€â”€ future\n",
      "     â”œâ”€â”€ depth (1,) <U4\n",
      "     â”œâ”€â”€ stocks (5, 60934, 158159) float32\n",
      "     â”œâ”€â”€ time (5,) int64\n",
      "     â”œâ”€â”€ x (158159,) float64\n",
      "     â””â”€â”€ y (60934,) float64\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path_recent = 'SOC_maps/Recent_Nov/'\n",
    "file_path_future = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "ds_name = 'stocks'\n",
    "\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2023\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2023, 2039, 5).astype(np.str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "year = \"2018\"\n",
    "time = pd.date_range(\"2018\", \"2023\", freq='A-DEC', name=\"time\")[0::5][0]\n",
    "\n",
    "# Save baseline in each scenario\n",
    "print(f'Year: {year}')   \n",
    "file_name = f'SOC_2018_4326.tif'\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path_recent+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "#xda = xda.where(xda != xda.attrs.get('nodatavals')[0]) \n",
    "\n",
    "# add time and depth coordinates\n",
    "xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "\n",
    "# convert to Dataset\n",
    "xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "\n",
    "# add depth coordinate\n",
    "xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "        \n",
    "# Save in S3\n",
    "for scenario in scenarios:\n",
    "    s3_path = f's3://soils-revealed/{scenario}.zarr'\n",
    "    print(f'Scenario: {scenario}')\n",
    "    \n",
    "    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "    xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "    #consolidate metadata at root\n",
    "    zarr.consolidate_metadata(store)\n",
    "    c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "    with zarr.open(store, mode='r') as z:\n",
    "        print(z.tree())\n",
    "                \n",
    "xda = None\n",
    "xds = None\n",
    "\n",
    "for scenario in scenarios:\n",
    "    s3_path = f's3://soils-revealed/{scenario}.zarr'\n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY[year]}_nov.tif'\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path_future+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        ## replace all values equal to 0 with np.nan\n",
    "        #if scenario == 'crop_I':\n",
    "        #    xda = xda.where(xda != 0.0) \n",
    "            \n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        \n",
    "        # add depth coordinate\n",
    "        xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "            \n",
    "        xda = None\n",
    "        xds = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2018\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 35.9 GiB for an array with shape (1, 60934, 158159) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     28\u001b[0m xda \u001b[39m=\u001b[39m rxr\u001b[39m.\u001b[39mopen_rasterio(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../../data/\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mband\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Remove tiff\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#os.remove(f'../../data/processed/raster_data/{file_name}')\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# replace all values equal to 0 with np.nan\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m#xda = xda.where(xda != xda.attrs.get('nodatavals')[0]) \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[39m# add time and depth coordinates\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m xda \u001b[39m=\u001b[39m xda\u001b[39m.\u001b[39;49massign_coords({\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m: time})\u001b[39m.\u001b[39;49mexpand_dims([\u001b[39m'\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     37\u001b[0m \u001b[39m# convert to Dataset\u001b[39;00m\n\u001b[1;32m     38\u001b[0m xds \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mDataset({ds_name: xda}, attrs\u001b[39m=\u001b[39mxda\u001b[39m.\u001b[39mattrs)\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/dataarray.py:2534\u001b[0m, in \u001b[0;36mDataArray.expand_dims\u001b[0;34m(self, dim, axis, **dim_kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m     dim \u001b[39m=\u001b[39m {cast(Hashable, dim): \u001b[39m1\u001b[39m}\n\u001b[1;32m   2533\u001b[0m dim \u001b[39m=\u001b[39m either_dict_or_kwargs(dim, dim_kwargs, \u001b[39m\"\u001b[39m\u001b[39mexpand_dims\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2534\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_temp_dataset()\u001b[39m.\u001b[39;49mexpand_dims(dim, axis)\n\u001b[1;32m   2535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_from_temp_dataset(ds)\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/dataset.py:4059\u001b[0m, in \u001b[0;36mDataset.expand_dims\u001b[0;34m(self, dim, axis, **dim_kwargs)\u001b[0m\n\u001b[1;32m   4057\u001b[0m         \u001b[39mfor\u001b[39;00m d, c \u001b[39min\u001b[39;00m zip_axis_dim:\n\u001b[1;32m   4058\u001b[0m             all_dims\u001b[39m.\u001b[39minsert(d, c)\n\u001b[0;32m-> 4059\u001b[0m         variables[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39;49mset_dims(\u001b[39mdict\u001b[39;49m(all_dims))\n\u001b[1;32m   4060\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4061\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m variables:\n\u001b[1;32m   4062\u001b[0m         \u001b[39m# If dims includes a label of a non-dimension coordinate,\u001b[39;00m\n\u001b[1;32m   4063\u001b[0m         \u001b[39m# it will be promoted to a 1D coordinate with a single value.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/variable.py:1724\u001b[0m, in \u001b[0;36mVariable.set_dims\u001b[0;34m(self, dims, shape)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     dims_map \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(dims, shape))\n\u001b[1;32m   1723\u001b[0m     tmp_shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(dims_map[d] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m expanded_dims)\n\u001b[0;32m-> 1724\u001b[0m     expanded_data \u001b[39m=\u001b[39m duck_array_ops\u001b[39m.\u001b[39mbroadcast_to(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, tmp_shape)\n\u001b[1;32m   1725\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1726\u001b[0m     expanded_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[(\u001b[39mNone\u001b[39;00m,) \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(expanded_dims) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim)]\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/variable.py:435\u001b[0m, in \u001b[0;36mVariable.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\n\u001b[1;32m    434\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, indexing\u001b[39m.\u001b[39mExplicitlyIndexed):\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49mget_duck_array()\n\u001b[1;32m    436\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/indexing.py:696\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_duck_array\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 696\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_cached()\n\u001b[1;32m    697\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray\u001b[39m.\u001b[39mget_duck_array()\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/indexing.py:690\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ensure_cached\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49mget_duck_array())\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/indexing.py:664\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_duck_array\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 664\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49mget_duck_array()\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/indexing.py:551\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_duck_array\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 551\u001b[0m     array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey]\n\u001b[1;32m    552\u001b[0m     \u001b[39m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[39m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[39m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[39m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/rioxarray/_io.py:423\u001b[0m, in \u001b[0;36mRasterioArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m indexing\u001b[39m.\u001b[39;49mexplicit_indexing_adapter(\n\u001b[1;32m    424\u001b[0m         key, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, indexing\u001b[39m.\u001b[39;49mIndexingSupport\u001b[39m.\u001b[39;49mOUTER, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem\n\u001b[1;32m    425\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/xarray/core/indexing.py:858\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[0;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \n\u001b[1;32m    838\u001b[0m \u001b[39mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    857\u001b[0m raw_key, numpy_indices \u001b[39m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[0;32m--> 858\u001b[0m result \u001b[39m=\u001b[39m raw_indexing_method(raw_key\u001b[39m.\u001b[39;49mtuple)\n\u001b[1;32m    859\u001b[0m \u001b[39mif\u001b[39;00m numpy_indices\u001b[39m.\u001b[39mtuple:\n\u001b[1;32m    860\u001b[0m     \u001b[39m# index the loaded np.ndarray\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     result \u001b[39m=\u001b[39m NumpyIndexingAdapter(result)[numpy_indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/rioxarray/_io.py:400\u001b[0m, in \u001b[0;36mRasterioArrayWrapper._getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvrt_params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     riods \u001b[39m=\u001b[39m WarpedVRT(riods, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvrt_params)\n\u001b[0;32m--> 400\u001b[0m out \u001b[39m=\u001b[39m riods\u001b[39m.\u001b[39;49mread(band_key, window\u001b[39m=\u001b[39;49mwindow, masked\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmasked)\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unsigned_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mastype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unsigned_dtype)\n",
      "File \u001b[0;32mrasterio/_io.pyx:590\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 35.9 GiB for an array with shape (1, 60934, 158159) and data type float32"
     ]
    }
   ],
   "source": [
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path_recent = 'SOC_maps/Recent_Nov/'\n",
    "file_path_future = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "ds_name = 'stocks'\n",
    "\n",
    "scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2023\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2023, 2039, 5).astype(str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "year = \"2018\"\n",
    "time = pd.date_range(\"2018\", \"2023\", freq='A-DEC', name=\"time\")[0::5][0]\n",
    "\n",
    "times = [times[-1]]\n",
    "years = [years[-1]]\n",
    "\n",
    "# Save baseline in each scenario\n",
    "print(f'Year: {year}')   \n",
    "file_name = f'SOC_2018_4326.tif'\n",
    "    \n",
    "## Download tiff\n",
    "#download_blob(bucket_name, file_path_recent+file_name, f'../../data/{file_name}')\n",
    "## Read tiff\n",
    "xda = rxr.open_rasterio(f'../../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "#os.remove(f'../../data/processed/raster_data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "#xda = xda.where(xda != xda.attrs.get('nodatavals')[0]) \n",
    "\n",
    "# add time and depth coordinates\n",
    "xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "\n",
    "# convert to Dataset\n",
    "xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "\n",
    "# add depth coordinate\n",
    "xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "# Save\n",
    "for scenario in scenarios:\n",
    "    path = f'../../data/processed/raster_data/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "\n",
    "    xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "    #consolidate metadata at root\n",
    "    zarr.consolidate_metadata(path)\n",
    "    with zarr.open(path, mode='r') as z:\n",
    "        print(z.tree())\n",
    "                \n",
    "xda = None\n",
    "xds = None\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = f'../../data/processed/raster_data/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY[year]}_nov.tif'\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path_future+file_name, f'../../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = rxr.open_rasterio(f'../../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../../data/{file_name}')\n",
    "        \n",
    "        ## replace all values equal to 0 with np.nan\n",
    "        #if scenario == 'crop_I':\n",
    "        #    xda = xda.where(xda != 0.0) \n",
    "            \n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        \n",
    "        # add depth coordinate\n",
    "        xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "\n",
    "        # Save\n",
    "        xds.attrs = {} # Clear the dataset's attributes\n",
    "        xds[ds_name].attrs = {} # Clear the variable's attributes\n",
    "        xds[ds_name].encoding.pop(\"add_offset\", None)  # Remove the 'add_offset' key if it exists\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "            \n",
    "        xda = None\n",
    "        xds = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Cover\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.googleapis.com/vizz-data-transfer/land-cover/\n",
    "\n",
    "**Data description:**\n",
    "The name structure of the files is `ESA_2000_ipcc.tif`:\n",
    "- YEAR: 2000 - 2018 \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/land-cover.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:29: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://soils-revealed/land-cover.zarr is consoldiated? True\n",
      "/\n",
      " â”œâ”€â”€ land-cover (1, 60934, 158159) uint8\n",
      " â”œâ”€â”€ time (1,) int64\n",
      " â”œâ”€â”€ x (158159,) float64\n",
      " â””â”€â”€ y (60934,) float64\n",
      "Year: 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:29: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://soils-revealed/land-cover.zarr is consoldiated? True\n",
      "/\n",
      " â”œâ”€â”€ land-cover (2, 60934, 158159) uint8\n",
      " â”œâ”€â”€ time (2,) int64\n",
      " â”œâ”€â”€ x (158159,) float64\n",
      " â””â”€â”€ y (60934,) float64\n",
      "CPU times: user 44.7 s, sys: 1min 51s, total: 2min 36s\n",
      "Wall time: 5min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/land-cover/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/land-cover.zarr'\n",
    "\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "group = 'land-cover'\n",
    "\n",
    "ds_name = 'land-cover'\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "times = [times[0], times[-1]]\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(str)\n",
    "years = [years[0], years[-1]]\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'land-cover/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'ESA_{year}_ipcc_res_align.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    ## Download tiff\n",
    "    #download_blob(bucket_name, file_path+file_name, f'../../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../../data/raw/raster_data/{file_name}', \n",
    "                           engine=\"rasterio\", chunks={\"x\": 4943, \"y\": 953}).squeeze().drop(\"band\")\n",
    "    \n",
    "    ## Remove tiff\n",
    "    #os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    ## add depth coordinate\n",
    "    #xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/land-cover-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:26: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      " â”œâ”€â”€ land-cover (1, 60934, 158159) uint8\n",
      " â”œâ”€â”€ time (1,) int64\n",
      " â”œâ”€â”€ x (158159,) float64\n",
      " â””â”€â”€ y (60934,) float64\n",
      "Year: 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:26: DeprecationWarning: open_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      " â”œâ”€â”€ land-cover (2, 60934, 158159) uint8\n",
      " â”œâ”€â”€ time (2,) int64\n",
      " â”œâ”€â”€ x (158159,) float64\n",
      " â””â”€â”€ y (60934,) float64\n",
      "CPU times: user 20.8 s, sys: 1min 49s, total: 2min 10s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/land-cover/'\n",
    "# Local path\n",
    "path = '../../data/processed/raster_data/land-cover.zarr' \n",
    "group = 'land-cover'\n",
    "\n",
    "ds_name = 'land-cover'\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "times = [times[0], times[-1]]\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(str)\n",
    "years = [years[0], years[-1]]\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'land-cover/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'ESA_{year}_ipcc_res_align.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    ## Download tiff\n",
    "    #download_blob(bucket_name, file_path+file_name, f'../../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../../data/raw/raster_data/{file_name}', \n",
    "                           engine=\"rasterio\", chunks={\"x\": 4943, \"y\": 953}).squeeze().drop(\"band\")\n",
    "    \n",
    "    ## Remove tiff\n",
    "    #os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    ## add depth coordinate\n",
    "    #xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        #xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        xds.to_zarr(store=path, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        #xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        xds.to_zarr(store=path, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` in memory\n",
    "\n",
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/soil-tnc-data.zarr/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "ds_name = 'stocks'\n",
    "depth = np.array(['0-30'])\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    \n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    if n == 0:\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    else:\n",
    "        xds = xr.concat([xds, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='time')\n",
    "        \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "xds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "#base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Save to zarr\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "ds_name = 'concentration'\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Year: {year}')\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"depth\": depth, \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depht\n",
    "        if depth == '0-5':\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        else:\n",
    "            xds_depth = xr.concat([xds_depth, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='depht')\n",
    "            \n",
    "    # select sub-area\n",
    "    xds_depth = xds_depth.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "        \n",
    "    # concatenate Datasets by time\n",
    "    if n == 0:\n",
    "        xds = xds_depth\n",
    "    else:\n",
    "        xds = xr.concat([xds, xds_depth], dim='time')\n",
    "        \n",
    "xds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-concentration'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "store = gc.get_mapper(root)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `xarray.Dataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "sotre = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Read Zarr file\n",
    "ds_s3 = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_s3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "ds_name = 'stocks'\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 1985, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=False, create=True)\n",
    "        #store = gc.get_mapper(root)\n",
    "        #xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=True, create=False)\n",
    "        #xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "ds_zarr = xr.open_zarr(local_path, group=group)\n",
    "ds_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to_zarr append with gcsmap does not work properly #3251](https://github.com/pydata/xarray/issues/3251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "74348868169075292b3e8230f0edfc006550d52ebfa36d277169a2af5362ab9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
