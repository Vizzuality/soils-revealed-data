{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `GeoTIFFs` in Google Cloud Storage to `Zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import zarr\n",
    "import rioxarray\n",
    "import gcsfs\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path \n",
    "env_path = Path('.') / '.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, blob_name, file_name):\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "        \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    blob.download_to_filename(file_name)\n",
    "    \n",
    "    print(\n",
    "        \"File {} downloaded to {}.\".format(\n",
    "            blob_name, file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From `GeoTIFFs` to `Zarr`\n",
    "\n",
    "We use the [xarray](http://xarray.pydata.org/en/stable/io.html#reading-and-writing-files) library to convert `GeoTIFFs` into `Zarr`. \n",
    "\n",
    "GeoTIFFs can be opened using [rasterio](http://xarray.pydata.org/en/stable/io.html#rasterio) with this xarray method: `xarray.open_rasterio`. Additionally, you can use [rioxarray](https://corteva.github.io/rioxarray/stable/) for reading GeoTiffs.\n",
    "\n",
    "To save `xarray.Datasets` as a `Zarr` we can us the [Xarrayâ€™s Zarr backend](http://xarray.pydata.org/en/stable/io.html#zarr). [Zarr](http://zarr.readthedocs.io/) is a Python package providing an implementation of chunked, compressed, N-dimensional arrays. Zarr has the ability to read and write xarray datasets directly from / to cloud storage buckets such as Amazon S3 and Google Cloud Storage.\n",
    "\n",
    "Xarray needs to read all of the zarr metadata when it opens a dataset. With version 2.3, Zarr will support a feature called consolidated metadata, which allows all metadata for the entire dataset to be stored with a single key (by default called `.zmetadata`). This can drastically speed up opening the store. To write consolidated metadata, pass the `consolidated=True` option to the `Dataset.to_zarr` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "        ## Save locally\n",
    "        #xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        ## consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(local_path)\n",
    "        #with zarr.open(local_path, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "        ## Save locally\n",
    "        #xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        ## consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(local_path)\n",
    "        #with zarr.open(local_path, mode='r') as z:\n",
    "        #    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm\n",
    "    \n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "# AWS S3 path\n",
    "#s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historic SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/SOC_maps/Historic/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOCS_DEPTH_year_YEAR_10km.tif`:\n",
    "- DEPTH: 0_30cm, 0_100cm, 0_200cm\n",
    "- YEAR: NoLU, 2010AD \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: NoLU\n",
      "Depth: 0-30\n",
      "Download tiff\n",
      "File SOC_maps/Historic/SOCS_0_30cm_year_NoLU_10km.tif downloaded to ../data/SOCS_0_30cm_year_NoLU_10km.tif.\n",
      "Read data\n",
      "Remove data\n",
      "Depth: 0-100\n",
      "Download tiff\n",
      "File SOC_maps/Historic/SOCS_0_100cm_year_NoLU_10km.tif downloaded to ../data/SOCS_0_100cm_year_NoLU_10km.tif.\n",
      "Read data\n",
      "Remove data\n",
      "Depth: 0-200\n",
      "Download tiff\n",
      "File SOC_maps/Historic/SOCS_0_200cm_year_NoLU_10km.tif downloaded to ../data/SOCS_0_200cm_year_NoLU_10km.tif.\n",
      "Read data\n",
      "Remove data\n",
      "/\n",
      " â””â”€â”€ historic\n",
      "     â”œâ”€â”€ depth (3,) object\n",
      "     â”œâ”€â”€ stocks (3, 1, 2160, 4320) float64\n",
      "     â”œâ”€â”€ time (1,) <U4\n",
      "     â”œâ”€â”€ x (4320,) float64\n",
      "     â””â”€â”€ y (2160,) float64\n",
      "Year: 2010AD\n",
      "Depth: 0-30\n",
      "Download tiff\n",
      "File SOC_maps/Historic/SOCS_0_30cm_year_2010AD_10km.tif downloaded to ../data/SOCS_0_30cm_year_2010AD_10km.tif.\n",
      "Read data\n",
      "Remove data\n",
      "Depth: 0-100\n",
      "Download tiff\n",
      "File SOC_maps/Historic/SOCS_0_100cm_year_2010AD_10km.tif downloaded to ../data/SOCS_0_100cm_year_2010AD_10km.tif.\n",
      "Read data\n",
      "Remove data\n",
      "Depth: 0-200\n",
      "Download tiff\n",
      "File SOC_maps/Historic/SOCS_0_200cm_year_2010AD_10km.tif downloaded to ../data/SOCS_0_200cm_year_2010AD_10km.tif.\n",
      "Read data\n",
      "Remove data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'store' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# Local path\n",
    "path = '../data/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Historic/'\n",
    "\n",
    "file_name = f'../data/SOCS_0_30cm_year_NoLU_10km.tif'\n",
    "blob_name = file_path+\"SOCS_0_30cm_year_NoLU_10km.tif\"\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into local directory\n",
    "    if i == 0:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` in memory\n",
    "\n",
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/soil-tnc-data.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "ds_name = 'stocks'\n",
    "depth = np.array(['0-30'])\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    \n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    if n == 0:\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    else:\n",
    "        xds = xr.concat([xds, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='time')\n",
    "        \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "#base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Save to zarr\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "ds_name = 'concentration'\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Year: {year}')\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"depth\": depth, \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depht\n",
    "        if depth == '0-5':\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        else:\n",
    "            xds_depth = xr.concat([xds_depth, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='depht')\n",
    "            \n",
    "    # select sub-area\n",
    "    xds_depth = xds_depth.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "        \n",
    "    # concatenate Datasets by time\n",
    "    if n == 0:\n",
    "        xds = xds_depth\n",
    "    else:\n",
    "        xds = xr.concat([xds, xds_depth], dim='time')\n",
    "        \n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-concentration'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "store = gc.get_mapper(root)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "sotre = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Read Zarr file\n",
    "ds_s3 = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "ds_name = 'stocks'\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 1985, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=False, create=True)\n",
    "        #store = gc.get_mapper(root)\n",
    "        #xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=True, create=False)\n",
    "        #xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "ds_zarr = xr.open_zarr(local_path, group=group)\n",
    "ds_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to_zarr append with gcsmap does not work properly #3251](https://github.com/pydata/xarray/issues/3251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
