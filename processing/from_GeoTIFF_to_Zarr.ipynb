{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `GeoTIFFs` in Google Cloud Storage to `Zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import zarr\n",
    "import rioxarray\n",
    "import gcsfs\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path \n",
    "env_path = Path('.') / '.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download_blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, blob_name, file_name):\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "        \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    blob.download_to_filename(file_name)\n",
    "    \n",
    "    print(\n",
    "        \"File {} downloaded to {}.\".format(\n",
    "            blob_name, file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upload_blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, blob_name, file_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            file_name, blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**change_compression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tiff = '../data/scenario_crop_MG_SOC_Y05.tif'\n",
    "output_tiff = '../data/scenario_crop_MG_SOC_Y05_new.tif'  \n",
    "\n",
    "def change_compression(input_tiff, output_tiff, compression='LZW'):\n",
    "    translateoptions = gdal.TranslateOptions(gdal.ParseCommandLine(f\"-of Gtiff -co COMPRESS={compression}\"))\n",
    "    gdal.Translate(output_tiff, input_tiff, options=translateoptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "scenarios = ['crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dYs = ['05', '10', '15', '20']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for dY in dYs:\n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY}.tif'\n",
    "        file_name_out = f'scenario_{scenario}_SOC_Y{dY}_new.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Check compression\n",
    "        dataset = gdal.OpenEx(f'../data/{file_name}')\n",
    "        md = dataset.GetMetadata('IMAGE_STRUCTURE')\n",
    "        \n",
    "        # Use dict.get method in case the metadata dict does not have a 'COMPRESSION' key\n",
    "        compression = md.get('COMPRESSION', None)\n",
    "        \n",
    "        if compression == 'ZSTD':\n",
    "            print(compression)\n",
    "            change_compression(f'../data/{file_name}', f'../data/{file_name_out}', compression='LZW')    \n",
    "            \n",
    "            os.rename(f'../data/{file_name_out}', f'../data/{file_name}')\n",
    "            # Upload tiff\n",
    "            upload_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        ## Remove tiff\n",
    "        os.remove(f'../data/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From `GeoTIFFs` to `Zarr`\n",
    "\n",
    "We use the [xarray](http://xarray.pydata.org/en/stable/io.html#reading-and-writing-files) library to convert `GeoTIFFs` into `Zarr`. \n",
    "\n",
    "GeoTIFFs can be opened using [rasterio](http://xarray.pydata.org/en/stable/io.html#rasterio) with this xarray method: `xarray.open_rasterio`. Additionally, you can use [rioxarray](https://corteva.github.io/rioxarray/stable/) for reading GeoTiffs.\n",
    "\n",
    "To save `xarray.Datasets` as a `Zarr` we can us the [Xarrayâ€™s Zarr backend](http://xarray.pydata.org/en/stable/io.html#zarr). [Zarr](http://zarr.readthedocs.io/) is a Python package providing an implementation of chunked, compressed, N-dimensional arrays. Zarr has the ability to read and write xarray datasets directly from / to cloud storage buckets such as Amazon S3 and Google Cloud Storage.\n",
    "\n",
    "Xarray needs to read all of the zarr metadata when it opens a dataset. With version 2.3, Zarr will support a feature called consolidated metadata, which allows all metadata for the entire dataset to be stored with a single key (by default called `.zmetadata`). This can drastically speed up opening the store. To write consolidated metadata, pass the `consolidated=True` option to the `Dataset.to_zarr` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "        ## Save locally\n",
    "        #xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        ## consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(local_path)\n",
    "        #with zarr.open(local_path, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "        ## Save locally\n",
    "        #xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        ## consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(local_path)\n",
    "        #with zarr.open(local_path, mode='r') as z:\n",
    "        #    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "# Local path\n",
    "path = '../data/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_stock/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm\n",
    "    \n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "# AWS S3 path\n",
    "#s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "# Local path\n",
    "path = '../data/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_concentration/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historic SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/SOC_maps/Historic/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOCS_DEPTH_year_YEAR_10km.tif`:\n",
    "- DEPTH: 0_30cm, 0_100cm, 0_200cm\n",
    "- YEAR: NoLU, 2010AD \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# Local path\n",
    "path = '../data/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Historic/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into local directory\n",
    "    if i == 0:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/SOC_maps/Recent/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_4326.tif`:\n",
    "- YEAR: 2000 - 2018 \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "\n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    print('Remove tiff.')\n",
    "    os.remove(f'../data/{file_name}')\n",
    "\n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent/'\n",
    "# Local path\n",
    "path = '../data/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/Future/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "Scenario file are named scenario_xxxx_yyyy_Yzz\n",
    "\n",
    "Where: \n",
    "  - xxxx = scenario names:\n",
    "    - crop.MG \n",
    "    - crop.MGI\n",
    "    - crop.I\n",
    "    - grass.part\n",
    "    - grass.full\n",
    "    - rewilding\n",
    "    - degradation.ForestToGrass\n",
    "    - degradation.ForestToCrop\n",
    "    - degradation.NoDeforestation\n",
    "  - yyyy = either dSOC (for change in SOC stocks) or SOC (for absolute SOC stocks)\n",
    "  - zz = years after change in land use or management (05, 10, 15, 20)\n",
    "\n",
    "All scenario rasters are in units of Mg C / ha to 30 cm depth\n",
    "\n",
    "**Scenarios**\n",
    "- crop_MG: Cropland, improved management only \n",
    "- crop_MGI: Cropland, improved management and inputs \n",
    "- crop_I: Cropland, improved inputs only \n",
    "- grass-part: Grassland, partial restoration \n",
    "- grass-full: Grassland, full restoration \n",
    "- rewilding: Rewilding \n",
    "- degradation-ForestToGrass: Degradation (includes deforestation to degraded grassland condition)\n",
    "- degradation-ForestToCrop: Degradation (includes deforestation to cropland)\n",
    "- degradation-NoDeforestation: Degradation (no deforestation)\n",
    "\n",
    "**Output data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2023\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2023, 2039, 5).astype(np.str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "# Save baseline in each scenario\n",
    "file_name = f'scenario_crop_I_SOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "soc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "soc = soc.where(soc != soc.attrs.get('nodatavals')[0]) \n",
    "        \n",
    "file_name = f'scenario_crop_I_dSOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "dsoc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "\n",
    "year = \"2018\"\n",
    "time = pd.date_range(\"2018\", \"2023\", freq='A-DEC', name=\"time\")[0::5][0]\n",
    "x_coords = [soc.coords['x'].values[0], soc.coords['x'].values[-1]]\n",
    "y_coords = [soc.coords['y'].values[0], soc.coords['y'].values[-1]]\n",
    "\n",
    "print(f'Year: {year}')\n",
    "for nx, x in enumerate(x_coords):\n",
    "    for ny, y in enumerate(y_coords):    \n",
    "        if nx == 0:\n",
    "            if ny == 0:     \n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "                # Save in S3\n",
    "                for scenario in scenarios:\n",
    "                    s3_path = f's3://soils-revealed/{scenario}.zarr'\n",
    "                    print(f'Scenario: {scenario}')\n",
    "                    \n",
    "                    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(path)\n",
    "                    c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "                    with zarr.open(store, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None  \n",
    "\n",
    "        else:\n",
    "            if ny == 0:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                    \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "                # Save in S3\n",
    "                for scenario in scenarios:\n",
    "                    s3_path = f's3://soils-revealed/{scenario}.zarr' \n",
    "                    print(f'Scenario: {scenario}')\n",
    "                    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "                    \n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=store, group=group, mode='a', append_dim='x', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(path)\n",
    "                    c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "                    print(f\"{s3_path} is consoldiated? {c}\")\n",
    "                    with zarr.open(store, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                       \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None   \n",
    "                soc = None\n",
    "                dsoc = None\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # AWS S3 path\n",
    "    s3_path = f's3://soils-revealed/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY[year]}.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "            \n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        \n",
    "        # add depth coordinate\n",
    "        xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "            \n",
    "        xda = None\n",
    "        xds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "#scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "scenarios = ['rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2023\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2023, 2039, 5).astype(np.str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "# Save baseline in each scenario\n",
    "file_name = f'scenario_crop_I_SOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "soc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "soc = soc.where(soc != soc.attrs.get('nodatavals')[0]) \n",
    "        \n",
    "file_name = f'scenario_crop_I_dSOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "dsoc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "\n",
    "year = \"2018\"\n",
    "time = pd.date_range(\"2018\", \"2023\", freq='A-DEC', name=\"time\")[0::5][0]\n",
    "x_coords = [soc.coords['x'].values[0], soc.coords['x'].values[-1]]\n",
    "y_coords = [soc.coords['y'].values[0], soc.coords['y'].values[-1]]\n",
    "\n",
    "print(f'Year: {year}')\n",
    "for nx, x in enumerate(x_coords):\n",
    "    for ny, y in enumerate(y_coords):    \n",
    "        if nx == 0:\n",
    "            if ny == 0:     \n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "                # Save\n",
    "                for scenario in scenarios:\n",
    "                    path = f'../data/{scenario}.zarr' \n",
    "                    print(f'Scenario: {scenario}')\n",
    "    \n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(path)\n",
    "                    with zarr.open(path, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None  \n",
    "\n",
    "        else:\n",
    "            if ny == 0:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                    \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "                # Save\n",
    "                for scenario in scenarios:\n",
    "                    path = f'../data/{scenario}.zarr' \n",
    "                    print(f'Scenario: {scenario}')\n",
    "                    \n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=path, group=group, mode='a', append_dim='x', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(path)\n",
    "                    with zarr.open(path, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                       \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None   \n",
    "                soc = None\n",
    "                dsoc = None\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = f'../data/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY[year]}.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "            \n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        \n",
    "        # add depth coordinate\n",
    "        xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "        \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "            \n",
    "        xda = None\n",
    "        xds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "file_name = f'scenario_crop_I_SOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "soc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "soc = soc.where(soc != soc.attrs.get('nodatavals')[0]) \n",
    "        \n",
    "file_name = f'scenario_crop_I_dSOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "dsoc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "#scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "scenarios = ['rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2018\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2018, 2039, 5).astype(np.str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "x_coords = [soc.coords['x'].values[0], soc.coords['x'].values[-1]]\n",
    "y_coords = [soc.coords['y'].values[0], soc.coords['y'].values[-1]]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = f'../data/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        print(f'Year: {year}')\n",
    "        if i == 0:\n",
    "            for nx, x in enumerate(x_coords):\n",
    "                for ny, y in enumerate(y_coords):    \n",
    "                    if nx == 0:\n",
    "                        if ny == 0:     \n",
    "                            soc_4 = soc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                            dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                            dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                            dsoc_4 = dsoc_4.fillna(0)\n",
    "                            \n",
    "                            xda = soc_4-dsoc_4\n",
    "                            \n",
    "                            # add time and depth coordinates\n",
    "                            xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "                            # convert to Dataset\n",
    "                            xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                            # add depth coordinate\n",
    "                            xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                            \n",
    "                        else:\n",
    "                            soc_4 = soc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                            dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                            dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                            dsoc_4 = dsoc_4.fillna(0)\n",
    "                            \n",
    "                            xda = soc_4-dsoc_4\n",
    "                            \n",
    "                            # add time and depth coordinates\n",
    "                            xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "                            # convert to Dataset\n",
    "                            xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                            # add depth coordinate\n",
    "                            xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                            \n",
    "                            # Save\n",
    "                            xr.concat([xds0,xds1], dim='y').to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "                            #consolidate metadata at root\n",
    "                            zarr.consolidate_metadata(path)\n",
    "                            with zarr.open(path, mode='r') as z:\n",
    "                                print(z.tree())\n",
    "                            \n",
    "                            soc_4 = None\n",
    "                            dsoc_4 = None\n",
    "                            xda = None\n",
    "                            xds0 = None\n",
    "                            xds1 = None  \n",
    "\n",
    "                    else:\n",
    "                        if ny == 0:\n",
    "                            soc_4 = soc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                            dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                            dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                            dsoc_4 = dsoc_4.fillna(0)\n",
    "                            \n",
    "                            xda = soc_4-dsoc_4\n",
    "                                \n",
    "                            # add time and depth coordinates\n",
    "                            xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "                            # convert to Dataset\n",
    "                            xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                            # add depth coordinate\n",
    "                            xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                                \n",
    "                        else:\n",
    "                            soc_4 = soc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                            dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                            dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                            dsoc_4 = dsoc_4.fillna(0)\n",
    "                            \n",
    "                            xda = soc_4-dsoc_4\n",
    "                                \n",
    "                            # add time and depth coordinates\n",
    "                            xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "                            # convert to Dataset\n",
    "                            xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                            # add depth coordinate\n",
    "                            xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                            \n",
    "                            # Save\n",
    "                            xr.concat([xds0,xds1], dim='y').to_zarr(store=path, group=group, mode='a', append_dim='x', consolidated=True)\n",
    "                            #consolidate metadata at root\n",
    "                            zarr.consolidate_metadata(path)\n",
    "                            with zarr.open(path, mode='r') as z:\n",
    "                                print(z.tree())\n",
    "                               \n",
    "                            soc_4 = None\n",
    "                            dsoc_4 = None\n",
    "                            xda = None\n",
    "                            xds0 = None\n",
    "                            xds1 = None   \n",
    "                            soc = None\n",
    "                            dsoc = None\n",
    "        else:\n",
    "            file_name = f'scenario_{scenario}_SOC_Y{dY[year]}.tif'\n",
    "            url = base_url + file_name\n",
    "            \n",
    "            # Download tiff\n",
    "            download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "            \n",
    "            # Read tiff\n",
    "            xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "            \n",
    "            # Remove tiff\n",
    "            os.remove(f'../data/{file_name}')\n",
    "                \n",
    "            # add time and depth coordinates\n",
    "            xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "            \n",
    "            # convert to Dataset\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            \n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "            # Save\n",
    "            xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "            # consolidate metadata at root\n",
    "            zarr.consolidate_metadata(path)\n",
    "            with zarr.open(path, mode='r') as z:\n",
    "                print(z.tree())\n",
    "                \n",
    "            xda = None\n",
    "            xds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` in memory\n",
    "\n",
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/soil-tnc-data.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "ds_name = 'stocks'\n",
    "depth = np.array(['0-30'])\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    \n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    if n == 0:\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    else:\n",
    "        xds = xr.concat([xds, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='time')\n",
    "        \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "#base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Save to zarr\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "ds_name = 'concentration'\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Year: {year}')\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"depth\": depth, \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depht\n",
    "        if depth == '0-5':\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        else:\n",
    "            xds_depth = xr.concat([xds_depth, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='depht')\n",
    "            \n",
    "    # select sub-area\n",
    "    xds_depth = xds_depth.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "        \n",
    "    # concatenate Datasets by time\n",
    "    if n == 0:\n",
    "        xds = xds_depth\n",
    "    else:\n",
    "        xds = xr.concat([xds, xds_depth], dim='time')\n",
    "        \n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-concentration'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "store = gc.get_mapper(root)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "sotre = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Read Zarr file\n",
    "ds_s3 = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "ds_name = 'stocks'\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 1985, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=False, create=True)\n",
    "        #store = gc.get_mapper(root)\n",
    "        #xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=True, create=False)\n",
    "        #xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "ds_zarr = xr.open_zarr(local_path, group=group)\n",
    "ds_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to_zarr append with gcsmap does not work properly #3251](https://github.com/pydata/xarray/issues/3251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
