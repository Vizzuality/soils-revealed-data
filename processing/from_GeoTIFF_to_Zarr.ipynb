{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `GeoTIFFs` in Google Cloud Storage to `Zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import zarr\n",
    "import rioxarray\n",
    "import gcsfs\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path \n",
    "env_path = Path('.') / '.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download_blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, blob_name, file_name):\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "        \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    blob.download_to_filename(file_name)\n",
    "    \n",
    "    print(\n",
    "        \"File {} downloaded to {}.\".format(\n",
    "            blob_name, file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upload_blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, blob_name, file_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv('PRIVATEKEY_PATH'))\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            file_name, blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upload_blob_s3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.session import Session as boto3_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.session import Session as boto3_session\n",
    "\n",
    "def upload_blob_s3(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    client = boto3_session.client(\"s3\",aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'),aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))\n",
    "    client.upload_file(source_file_name, bucket_name, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3_session.client(\"s3\",aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'),aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**change_compression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tiff = '../data/scenario_crop_MG_SOC_Y05.tif'\n",
    "output_tiff = '../data/scenario_crop_MG_SOC_Y05_new.tif'  \n",
    "\n",
    "def change_compression(input_tiff, output_tiff, compression='LZW'):\n",
    "    translateoptions = gdal.TranslateOptions(gdal.ParseCommandLine(f\"-of Gtiff -co COMPRESS={compression}\"))\n",
    "    gdal.Translate(output_tiff, input_tiff, options=translateoptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "scenarios = ['crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dYs = ['05', '10', '15', '20']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for dY in dYs:\n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY}.tif'\n",
    "        file_name_out = f'scenario_{scenario}_SOC_Y{dY}_new.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Check compression\n",
    "        dataset = gdal.OpenEx(f'../data/{file_name}')\n",
    "        md = dataset.GetMetadata('IMAGE_STRUCTURE')\n",
    "        \n",
    "        # Use dict.get method in case the metadata dict does not have a 'COMPRESSION' key\n",
    "        compression = md.get('COMPRESSION', None)\n",
    "        \n",
    "        if compression == 'ZSTD':\n",
    "            print(compression)\n",
    "            change_compression(f'../data/{file_name}', f'../data/{file_name_out}', compression='LZW')    \n",
    "            \n",
    "            os.rename(f'../data/{file_name_out}', f'../data/{file_name}')\n",
    "            # Upload tiff\n",
    "            upload_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        ## Remove tiff\n",
    "        os.remove(f'../data/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From `GeoTIFFs` to `Zarr`\n",
    "\n",
    "We use the [xarray](http://xarray.pydata.org/en/stable/io.html#reading-and-writing-files) library to convert `GeoTIFFs` into `Zarr`. \n",
    "\n",
    "GeoTIFFs can be opened using [rasterio](http://xarray.pydata.org/en/stable/io.html#rasterio) with this xarray method: `xarray.open_rasterio`. Additionally, you can use [rioxarray](https://corteva.github.io/rioxarray/stable/) for reading GeoTiffs.\n",
    "\n",
    "To save `xarray.Datasets` as a `Zarr` we can us the [Xarray’s Zarr backend](http://xarray.pydata.org/en/stable/io.html#zarr). [Zarr](http://zarr.readthedocs.io/) is a Python package providing an implementation of chunked, compressed, N-dimensional arrays. Zarr has the ability to read and write xarray datasets directly from / to cloud storage buckets such as Amazon S3 and Google Cloud Storage.\n",
    "\n",
    "Xarray needs to read all of the zarr metadata when it opens a dataset. With version 2.3, Zarr will support a feature called consolidated metadata, which allows all metadata for the entire dataset to be stored with a single key (by default called `.zmetadata`). This can drastically speed up opening the store. To write consolidated metadata, pass the `consolidated=True` option to the `Dataset.to_zarr` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "        ## Save locally\n",
    "        #xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        ## consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(local_path)\n",
    "        #with zarr.open(local_path, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "        \n",
    "        ## Save locally\n",
    "        #xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        ## consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(local_path)\n",
    "        #with zarr.open(local_path, mode='r') as z:\n",
    "        #    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "# Local path\n",
    "path = '../data/experimental-dataset.zarr' \n",
    "group = 'stocks'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 1983, 1).astype(np.str)#np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_stock/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    ## save zarr into Google Cloud Storage bucket\n",
    "    #if i == 0:   \n",
    "    #    # Save\n",
    "    #    xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "    #    #consolidate metadata at root\n",
    "    #    zarr.consolidate_metadata(path)\n",
    "    #    with zarr.open(path, mode='r') as z:\n",
    "    #        print(z.tree())\n",
    "#\n",
    "    #else:\n",
    "    #    # Save\n",
    "    #    xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "    #    # consolidate metadata at root\n",
    "    #    zarr.consolidate_metadata(path)\n",
    "    #    with zarr.open(path, mode='r') as z:\n",
    "    #        print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm\n",
    "    \n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 1982\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1982_q0.5_d2.5.tif downloaded to ../data/SOC_1982_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1982_q0.5_d10.tif downloaded to ../data/SOC_1982_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1982_q0.5_d22.5.tif downloaded to ../data/SOC_1982_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1982_q0.5_d45.tif downloaded to ../data/SOC_1982_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1982_q0.5_d80.tif downloaded to ../data/SOC_1982_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1982_q0.5_d150.tif downloaded to ../data/SOC_1982_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 1, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (1,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1983\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1983_q0.5_d2.5.tif downloaded to ../data/SOC_1983_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1983_q0.5_d10.tif downloaded to ../data/SOC_1983_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1983_q0.5_d22.5.tif downloaded to ../data/SOC_1983_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1983_q0.5_d45.tif downloaded to ../data/SOC_1983_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1983_q0.5_d80.tif downloaded to ../data/SOC_1983_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1983_q0.5_d150.tif downloaded to ../data/SOC_1983_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 2, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (2,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1984\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1984_q0.5_d2.5.tif downloaded to ../data/SOC_1984_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1984_q0.5_d10.tif downloaded to ../data/SOC_1984_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1984_q0.5_d22.5.tif downloaded to ../data/SOC_1984_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1984_q0.5_d45.tif downloaded to ../data/SOC_1984_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1984_q0.5_d80.tif downloaded to ../data/SOC_1984_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1984_q0.5_d150.tif downloaded to ../data/SOC_1984_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 3, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (3,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1985\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1985_q0.5_d2.5.tif downloaded to ../data/SOC_1985_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1985_q0.5_d10.tif downloaded to ../data/SOC_1985_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1985_q0.5_d22.5.tif downloaded to ../data/SOC_1985_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1985_q0.5_d45.tif downloaded to ../data/SOC_1985_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1985_q0.5_d80.tif downloaded to ../data/SOC_1985_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1985_q0.5_d150.tif downloaded to ../data/SOC_1985_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 4, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (4,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1986\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1986_q0.5_d2.5.tif downloaded to ../data/SOC_1986_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1986_q0.5_d10.tif downloaded to ../data/SOC_1986_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1986_q0.5_d22.5.tif downloaded to ../data/SOC_1986_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1986_q0.5_d45.tif downloaded to ../data/SOC_1986_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1986_q0.5_d80.tif downloaded to ../data/SOC_1986_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1986_q0.5_d150.tif downloaded to ../data/SOC_1986_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 5, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (5,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1987\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1987_q0.5_d2.5.tif downloaded to ../data/SOC_1987_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1987_q0.5_d10.tif downloaded to ../data/SOC_1987_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1987_q0.5_d22.5.tif downloaded to ../data/SOC_1987_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1987_q0.5_d45.tif downloaded to ../data/SOC_1987_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1987_q0.5_d80.tif downloaded to ../data/SOC_1987_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1987_q0.5_d150.tif downloaded to ../data/SOC_1987_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 6, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (6,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1988\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1988_q0.5_d2.5.tif downloaded to ../data/SOC_1988_q0.5_d2.5.tif.\n",
      "Depth: 5-15\n",
      "File SOC_maps/SOC_concentration/SOC_1988_q0.5_d10.tif downloaded to ../data/SOC_1988_q0.5_d10.tif.\n",
      "Depth: 15-30\n",
      "File SOC_maps/SOC_concentration/SOC_1988_q0.5_d22.5.tif downloaded to ../data/SOC_1988_q0.5_d22.5.tif.\n",
      "Depth: 30-60\n",
      "File SOC_maps/SOC_concentration/SOC_1988_q0.5_d45.tif downloaded to ../data/SOC_1988_q0.5_d45.tif.\n",
      "Depth: 60-100\n",
      "File SOC_maps/SOC_concentration/SOC_1988_q0.5_d80.tif downloaded to ../data/SOC_1988_q0.5_d80.tif.\n",
      "Depth: 100-200\n",
      "File SOC_maps/SOC_concentration/SOC_1988_q0.5_d150.tif downloaded to ../data/SOC_1988_q0.5_d150.tif.\n",
      "s3://soils-revealed/experimental-dataset.zarr is consoldiated? True\n",
      "/\n",
      " ├── concentration\n",
      " │   ├── concentration (6, 7, 15589, 9800) float64\n",
      " │   ├── depth (6,) object\n",
      " │   ├── time (7,) int64\n",
      " │   ├── x (9800,) float64\n",
      " │   └── y (15589,) float64\n",
      " └── stocks\n",
      "     ├── depth (1,) <U4\n",
      "     ├── stocks (36, 13883, 9872) float32\n",
      "     ├── time (36,) int64\n",
      "     ├── x (9872,) float64\n",
      "     └── y (13883,) float64\n",
      "Year: 1989\n",
      "Depth: 0-5\n",
      "File SOC_maps/SOC_concentration/SOC_1989_q0.5_d2.5.tif downloaded to ../data/SOC_1989_q0.5_d2.5.tif.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m             variable = (\n\u001b[0;32m-> 2598\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2599\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreflexive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/nputils.py\u001b[0m in \u001b[0;36marray_ne\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"elementwise comparison failed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ensure_bool_is_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m             \u001b[0mself_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_compat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2031\u001b[0m             \u001b[0mkeep_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_keep_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m             \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep_attrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m_broadcast_compat_data\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0;31m# rely on numpy broadcasting rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2344\u001b[0;31m         \u001b[0mself_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2345\u001b[0m         \u001b[0mother_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;34m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_as_array_or_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mTODO\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0monce\u001b[0m \u001b[0mthese\u001b[0m \u001b[0missues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m_ensure_cached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumpyIndexingAdapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumpyIndexingAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/backends/rasterio_.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         return indexing.explicit_indexing_adapter(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexingSupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mexplicit_indexing_adapter\u001b[0;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \"\"\"\n\u001b[1;32m    836\u001b[0m     \u001b[0mraw_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompose_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_indexing_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnumpy_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# index the loaded np.ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoenv/lib/python3.8/site-packages/xarray/backends/rasterio_.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvrt_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mriods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWarpedVRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mriods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvrt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mriods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msqueeze_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "# AWS S3 path\n",
    "#s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "s3_path = 's3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_concentration/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        url = base_url + file_name\n",
    "         \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/experimental-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "# Local path\n",
    "path = '../data/experimental-dataset.zarr' \n",
    "group = 'concentration'\n",
    "\n",
    "ds_name = 'concentration'\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 1983, 1).astype(np.str)#np.arange(1982, 2018, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/SOC_concentration/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-5':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    ## save zarr into Google Cloud Storage bucket\n",
    "    #if i == 0:\n",
    "    #    # Save\n",
    "    #    xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "    #    #consolidate metadata at root\n",
    "    #    zarr.consolidate_metadata(path)\n",
    "    #    with zarr.open(path, mode='r') as z:\n",
    "    #        print(z.tree())\n",
    "#\n",
    "    #else:\n",
    "    #    # Save\n",
    "    #    xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "    #    # consolidate metadata at root\n",
    "    #    zarr.consolidate_metadata(path)\n",
    "    #    with zarr.open(path, mode='r') as z:\n",
    "    #        print(z.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Zarr file\n",
    "ds = xr.open_zarr(store=path, group=group, consolidated=True)\n",
    "\n",
    "# Change coordinates names\n",
    "ds = ds.rename({'x': 'lon', 'y': 'lat'})\n",
    "\n",
    "# Change depth coord from 0 to 1 dimensional array\n",
    "depths = ds.coords.get('depth').values\n",
    "if depths.ndim == 0: \n",
    "    ds = ds.squeeze().drop(\"depth\")\n",
    "    ds = ds.assign_coords({\"depth\": np.array([depths])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historic SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/SOC_maps/Historic/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOCS_DEPTH_year_YEAR_10km.tif`:\n",
    "- DEPTH: 0_30cm, 0_100cm, 0_200cm\n",
    "- YEAR: NoLU, 2010AD \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Historic/'\n",
    "# Local path\n",
    "path = '../data/global-dataset.zarr' \n",
    "group = 'historic'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "depths = {'0-30': '0_30', '0-100': '0_100', '0-200': '0_200'}\n",
    "years = ['NoLU', '2010AD']\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Historic/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Depth: {depth}')\n",
    "        file_name = 'SOCS_' + dname +'cm_year_' + year +'_10km.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != -32767.0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": year}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depth\n",
    "        if depth == '0-30':\n",
    "            xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds = xds.assign_coords({\"depth\": np.array([depth])})\n",
    "        else:\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "            # add depth coordinate\n",
    "            xds_depth = xds_depth.assign_coords({\"depth\": np.array([depth])})\n",
    "            \n",
    "            xds = xr.concat([xds, xds_depth], dim='depth')\n",
    "        \n",
    "    \n",
    "    # save zarr into local directory\n",
    "    if i == 0:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/SOC_maps/Recent/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_4326.tif`:\n",
    "- YEAR: 2000 - 2018 \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://soils-revealed.s3.amazonaws.com/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent/'\n",
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "\n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    print('Remove tiff.')\n",
    "    os.remove(f'../data/{file_name}')\n",
    "\n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Recent/'\n",
    "# Local path\n",
    "path = '../data/global-dataset.zarr' \n",
    "group = 'recent'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "times = pd.date_range(\"2000\", \"2019\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(2000, 2019, 1).astype(np.str)\n",
    "\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Recent/'\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    file_name = f'SOC_{year}_4326.tif'\n",
    "    url = base_url + file_name\n",
    "    \n",
    "    # Download tiff\n",
    "    download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "    \n",
    "    # Read tiff\n",
    "    xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "    \n",
    "    # Remove tiff\n",
    "    os.remove(f'../data/{file_name}')\n",
    "        \n",
    "    # add time and depth coordinates\n",
    "    #xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # add depth coordinate\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "    \n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:   \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "        #consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "\n",
    "    else:\n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/Future/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "Scenario file are named scenario_xxxx_yyyy_Yzz\n",
    "\n",
    "Where: \n",
    "  - xxxx = scenario names:\n",
    "    - crop.MG \n",
    "    - crop.MGI\n",
    "    - crop.I\n",
    "    - grass.part\n",
    "    - grass.full\n",
    "    - rewilding\n",
    "    - degradation.ForestToGrass\n",
    "    - degradation.ForestToCrop\n",
    "    - degradation.NoDeforestation\n",
    "  - yyyy = either dSOC (for change in SOC stocks) or SOC (for absolute SOC stocks)\n",
    "  - zz = years after change in land use or management (05, 10, 15, 20)\n",
    "\n",
    "All scenario rasters are in units of Mg C / ha to 30 cm depth\n",
    "\n",
    "**Scenarios**\n",
    "- crop_MG: Cropland, improved management only \n",
    "- crop_MGI: Cropland, improved management and inputs \n",
    "- crop_I: Cropland, improved inputs only \n",
    "- grass-part: Grassland, partial restoration \n",
    "- grass-full: Grassland, full restoration \n",
    "- rewilding: Rewilding \n",
    "- degradation-ForestToGrass: Degradation (includes deforestation to degraded grassland condition)\n",
    "- degradation-ForestToCrop: Degradation (includes deforestation to cropland)\n",
    "- degradation-NoDeforestation: Degradation (no deforestation)\n",
    "\n",
    "**Output data location:**\n",
    "\n",
    "https://soils-revealed.s3.amazonaws.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2023\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2023, 2039, 5).astype(np.str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "# Save baseline in each scenario\n",
    "file_name = f'scenario_crop_I_SOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "soc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "soc = soc.where(soc != soc.attrs.get('nodatavals')[0]) \n",
    "        \n",
    "file_name = f'scenario_crop_I_dSOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "dsoc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "\n",
    "year = \"2018\"\n",
    "time = pd.date_range(\"2018\", \"2023\", freq='A-DEC', name=\"time\")[0::5][0]\n",
    "x_coords = [soc.coords['x'].values[0], soc.coords['x'].values[-1]]\n",
    "y_coords = [soc.coords['y'].values[0], soc.coords['y'].values[-1]]\n",
    "\n",
    "print(f'Year: {year}')\n",
    "for nx, x in enumerate(x_coords):\n",
    "    for ny, y in enumerate(y_coords):    \n",
    "        if nx == 0:\n",
    "            if ny == 0:     \n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "                # Save in S3\n",
    "                for scenario in scenarios:\n",
    "                    s3_path = f's3://soils-revealed/{scenario}.zarr'\n",
    "                    print(f'Scenario: {scenario}')\n",
    "                    \n",
    "                    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(store)\n",
    "                    c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "                    with zarr.open(store, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None  \n",
    "\n",
    "        else:\n",
    "            if ny == 0:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                    \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "                # Save in S3\n",
    "                for scenario in scenarios:\n",
    "                    s3_path = f's3://soils-revealed/{scenario}.zarr' \n",
    "                    print(f'Scenario: {scenario}')\n",
    "                    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "                    \n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=store, group=group, mode='a', append_dim='x', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(store)\n",
    "                    c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "                    print(f\"{s3_path} is consoldiated? {c}\")\n",
    "                    with zarr.open(store, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                       \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None   \n",
    "                soc = None\n",
    "                dsoc = None\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # AWS S3 path\n",
    "    s3_path = f's3://soils-revealed/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY[year]}.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "            \n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        \n",
    "        # add depth coordinate\n",
    "        xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "            \n",
    "        # Save in S3\n",
    "        store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "        xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(store)\n",
    "        c = s3.exists(f\"{s3_path}/.zmetadata\")\n",
    "        print(f\"{s3_path} is consoldiated? {c}\")\n",
    "        with zarr.open(store, mode='r') as z:\n",
    "            print(z.tree())\n",
    "            \n",
    "        xda = None\n",
    "        xds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output data location:**\n",
    " \n",
    "../data/global-dataset.zarr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/Future/'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "file_path = 'SOC_maps/Future/'\n",
    "\n",
    "group = 'future'\n",
    "\n",
    "ds_name = 'stocks'\n",
    "\n",
    "#scenarios = ['crop_I', 'crop_MG', 'crop_MGI', 'grass_part', 'grass_full', 'rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "scenarios = ['rewilding', 'degradation_ForestToGrass', 'degradation_ForestToCrop', 'degradation_NoDeforestation']\n",
    "dY = {'2023': '05', '2028': '10', '2033': '15', '2038': '20'}\n",
    "times = pd.date_range(\"2023\", \"2039\", freq='A-DEC', name=\"time\")[0::5]\n",
    "years = np.arange(2023, 2039, 5).astype(np.str)\n",
    "\n",
    "depth = ['0-30']\n",
    "\n",
    "# Save baseline in each scenario\n",
    "file_name = f'scenario_crop_I_SOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "soc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "# replace all values equal to 0 with np.nan\n",
    "soc = soc.where(soc != soc.attrs.get('nodatavals')[0]) \n",
    "        \n",
    "file_name = f'scenario_crop_I_dSOC_Y05.tif'\n",
    "url = base_url + file_name\n",
    "    \n",
    "## Download tiff\n",
    "download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "## Read tiff\n",
    "dsoc = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "# Remove tiff\n",
    "os.remove(f'../data/{file_name}')\n",
    "\n",
    "year = \"2018\"\n",
    "time = pd.date_range(\"2018\", \"2023\", freq='A-DEC', name=\"time\")[0::5][0]\n",
    "x_coords = [soc.coords['x'].values[0], soc.coords['x'].values[-1]]\n",
    "y_coords = [soc.coords['y'].values[0], soc.coords['y'].values[-1]]\n",
    "\n",
    "print(f'Year: {year}')\n",
    "for nx, x in enumerate(x_coords):\n",
    "    for ny, y in enumerate(y_coords):    \n",
    "        if nx == 0:\n",
    "            if ny == 0:     \n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(x, 0.), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "                # Save\n",
    "                for scenario in scenarios:\n",
    "                    path = f'../data/{scenario}.zarr' \n",
    "                    print(f'Scenario: {scenario}')\n",
    "    \n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=path, group=group, mode='w', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(path)\n",
    "                    with zarr.open(path, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None  \n",
    "\n",
    "        else:\n",
    "            if ny == 0:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(y, 0.)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds0 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds0 = xds0.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                    \n",
    "            else:\n",
    "                soc_4 = soc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc.sel(x=slice(0., x), y=slice(0., y)).copy()\n",
    "                dsoc_4 = dsoc_4.where(dsoc_4 != dsoc_4.values.min())\n",
    "                dsoc_4 = dsoc_4.fillna(0)\n",
    "                \n",
    "                xda = soc_4-dsoc_4\n",
    "                    \n",
    "                # add time and depth coordinates\n",
    "                xda = xda.assign_coords({\"time\": time}).expand_dims(['time'])\n",
    "                # convert to Dataset\n",
    "                xds1 = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "                # add depth coordinate\n",
    "                xds1 = xds1.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "                \n",
    "                # Save\n",
    "                for scenario in scenarios:\n",
    "                    path = f'../data/{scenario}.zarr' \n",
    "                    print(f'Scenario: {scenario}')\n",
    "                    \n",
    "                    xr.concat([xds0,xds1], dim='y').to_zarr(store=path, group=group, mode='a', append_dim='x', consolidated=True)\n",
    "                    #consolidate metadata at root\n",
    "                    zarr.consolidate_metadata(path)\n",
    "                    with zarr.open(path, mode='r') as z:\n",
    "                        print(z.tree())\n",
    "                       \n",
    "                soc_4 = None\n",
    "                dsoc_4 = None\n",
    "                xda = None\n",
    "                xds0 = None\n",
    "                xds1 = None   \n",
    "                soc = None\n",
    "                dsoc = None\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = f'../data/{scenario}.zarr' \n",
    "    print(f'Scenario: {scenario}')\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        file_name = f'scenario_{scenario}_SOC_Y{dY[year]}.tif'\n",
    "        url = base_url + file_name\n",
    "        \n",
    "        # Download tiff\n",
    "        download_blob(bucket_name, file_path+file_name, f'../data/{file_name}')\n",
    "        \n",
    "        # Read tiff\n",
    "        xda = xr.open_rasterio(f'../data/{file_name}').squeeze().drop(\"band\")\n",
    "        \n",
    "        # Remove tiff\n",
    "        os.remove(f'../data/{file_name}')\n",
    "            \n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"time\": times[i]}).expand_dims(['time'])\n",
    "        \n",
    "        # convert to Dataset\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        \n",
    "        # add depth coordinate\n",
    "        xds = xds.assign_coords({\"depth\": np.array([depth[0]])})\n",
    "        \n",
    "        # Save\n",
    "        xds.to_zarr(store=path, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(path)\n",
    "        with zarr.open(path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "            \n",
    "        xda = None\n",
    "        xds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create `xarray.Dataset` in memory\n",
    "\n",
    "### Argentina SOC stocks dataset\n",
    "\n",
    "**Data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `Feb19_cstocks_YEAR_030_ll.tif`:\n",
    "- YEAR: 1982-2017\n",
    "- The stocks were calculated in the 0 to 30 cm interval. \n",
    "\n",
    "**Output data location:**\n",
    " \n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/soil-tnc-data.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "ds_name = 'stocks'\n",
    "depth = np.array(['0-30'])\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    \n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    if n == 0:\n",
    "        xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    else:\n",
    "        xds = xr.concat([xds, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='time')\n",
    "        \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "#base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Save to zarr\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argentina SOC concentration dataset\n",
    "\n",
    "**Input data location:**\n",
    "\n",
    "https://storage.cloud.google.com/vizz-data-transfer/SOC_maps/\n",
    "\n",
    "**Data description:**\n",
    "\n",
    "The name structure of the files is `SOC_YEAR_qQUANTILE_dDEPTH.tif`:\n",
    "\n",
    "- YEAR: 1982-2017\n",
    "- QUANTILE: 0.05,0.5,0.95 percentiles\n",
    "- DEPTH:\n",
    "    - 2.5 --> for the interval 0-5cm\n",
    "    - 10 --> for the interval 5-15cm\n",
    "    - 22.5 --> for the interval 15-30cm\n",
    "    - 45 --> for the interval 30-60cm\n",
    "    - 80 --> for the interval 60-100cm\n",
    "    - 150 --> for the interval 100-200cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_concentration/'\n",
    "ds_name = 'concentration'\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depths = {'0-5': '2.5', '5-15': '10', '15-30': '22.5', '30-60': '45', '60-100': '80', '100-200': '150'}\n",
    "years = np.arange(1982, 1984, 1).astype(np.str)\n",
    "\n",
    "for n, year in enumerate(years):\n",
    "    for depth,dname in depths.items():\n",
    "        print(f'Year: {year}')\n",
    "        print(f'Depth: {depth}')\n",
    "        url = base_url + 'SOC_' + year + '_q0.5_d'+ dname + '.tif'\n",
    "        \n",
    "        xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "        \n",
    "        # replace all values equal to 0 with np.nan\n",
    "        xda = xda.where(xda != 0) \n",
    "\n",
    "        # add time and depth coordinates\n",
    "        xda = xda.assign_coords({\"depth\": depth, \"time\": times[n]}).expand_dims(['depht', 'time'])\n",
    "        \n",
    "        # convert to Dataset and concatenate by depht\n",
    "        if depth == '0-5':\n",
    "            xds_depth = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "        else:\n",
    "            xds_depth = xr.concat([xds_depth, xr.Dataset({ds_name: xda}, attrs=xda.attrs)], dim='depht')\n",
    "            \n",
    "    # select sub-area\n",
    "    xds_depth = xds_depth.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "        \n",
    "    # concatenate Datasets by time\n",
    "    if n == 0:\n",
    "        xds = xds_depth\n",
    "    else:\n",
    "        xds = xr.concat([xds, xds_depth], dim='time')\n",
    "        \n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `xarray.Dataset` as `Zarr` in Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-concentration'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "# Save in GCS\n",
    "store = gc.get_mapper(root, check=False, create=True)\n",
    "store = gc.get_mapper(root)\n",
    "xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "# consolidate metadata at root\n",
    "zarr.consolidate_metadata(store)\n",
    "c = gc.exists(f\"{root}/.zmetadata\")\n",
    "print(f\"{root} is consoldiated? {c}\")\n",
    "with zarr.open(store, mode='r') as z:\n",
    "    print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 path\n",
    "s3_path = 's3://soils-revealed/soil-data.zarr' \n",
    "group = 'experimental-dataset-stock'\n",
    "# Initilize the S3 file system\n",
    "s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "sotre = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "# Read Zarr file\n",
    "ds_s3 = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_path = '../data/soil-data.zarr'\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "ds_name = 'stocks'\n",
    "base_url = 'https://storage.googleapis.com/vizz-data-transfer/SOC_maps/SOC_stock/'\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "\n",
    "times = pd.date_range(\"1982\", \"2018\", freq='A-DEC', name=\"time\")\n",
    "depth = ['0-30']\n",
    "years = np.arange(1982, 1985, 1).astype(np.str)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'Year: {year}')\n",
    "    url = base_url + 'Feb19_cstocks_' + year + '_030_ll.tif'\n",
    "    xda = xr.open_rasterio(url).squeeze().drop(\"band\")\n",
    "    \n",
    "    # replace all values equal to -9999 with np.nan\n",
    "    xda = xda.where(xda != -9999.) \n",
    "    \n",
    "    # add time and depth coordinates\n",
    "    xda = xda.assign_coords({\"depth\": depth[0], \"time\": times[i]}).expand_dims(['depht', 'time'])\n",
    "    \n",
    "    # convert to Dataset\n",
    "    xds = xr.Dataset({ds_name: xda}, attrs=xda.attrs)\n",
    "    \n",
    "    # select sub-area\n",
    "    #xds = xds.isel(x=slice(2000, 2100), y=slice(4000, 4100))\n",
    "    \n",
    "    # save zarr into Google Cloud Storage bucket\n",
    "    if i == 0:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=False, create=True)\n",
    "        #store = gc.get_mapper(root)\n",
    "        #xds.to_zarr(store=store, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, mode='w', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())\n",
    "    else:\n",
    "        # Save in GCS\n",
    "        #store = gc.get_mapper(root, check=True, create=False)\n",
    "        #xds.to_zarr(store=store, group=group, mode='a', append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        #zarr.consolidate_metadata(store)\n",
    "        #c = gc.exists(f\"{root}/.zmetadata\")\n",
    "        #print(f\"{root} is consoldiated? {c}\")\n",
    "        #with zarr.open(store, mode='r') as z:\n",
    "        #    print(z.tree())\n",
    "        \n",
    "        # Save locally\n",
    "        xds.to_zarr(local_path, group=group, append_dim='time', consolidated=True)\n",
    "        # consolidate metadata at root\n",
    "        zarr.consolidate_metadata(local_path)\n",
    "        with zarr.open(local_path, mode='r') as z:\n",
    "            print(z.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read `xarray.Dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../data/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "ds_zarr = xr.open_zarr(local_path, group=group)\n",
    "ds_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "# Check zarr is consolidated\n",
    "#consolidated = gc.exists(f'{root}/.zmetadata')\n",
    "# Cache the zarr store\n",
    "#cache = zarr.LRUStoreCache(store, max_size=None)\n",
    "# Return cached zarr group\n",
    "ds_gcs = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "ds_gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to_zarr append with gcsmap does not work properly #3251](https://github.com/pydata/xarray/issues/3251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
