{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soils Revealed precalculations with `Zarrs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>\n",
    "### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler, visualize\n",
    "from xhistogram.xarray import histogram\n",
    "from rasterio import features\n",
    "import zarr\n",
    "import rioxarray\n",
    "import regionmask\n",
    "import gcsfs\n",
    "from geocube.api.core import make_geocube\n",
    "import shapely.wkb \n",
    "from shapely.ops import cascaded_union\n",
    "import json\n",
    "import requests\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path \n",
    "env_path = Path('.') / '.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()  # start distributed scheduler locally.  Launch dashboard\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='utils'></a>\n",
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df_from_carto'></a>\n",
    "**df_from_carto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_carto(account, query):\n",
    "    \"\"\"\n",
    "    It gets data by querying a carto table and converts it into a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    urlCarto = f\"https://{account}.carto.com/api/v2/sql\"\n",
    "    \n",
    "    sql = {\"q\": query}\n",
    "    r = requests.get(urlCarto, params=sql)\n",
    "    \n",
    "    data = r.json()\n",
    "    \n",
    "    df = gpd.GeoDataFrame(data.get(\"rows\"))\n",
    "    if 'the_geom' in df.columns:\n",
    "        # Change geometry from WKB to WKT format\n",
    "        df['geometry'] = df.apply(lambda x: shapely.wkb.loads(x['the_geom'],hex=True), axis=1 )\n",
    "        df.drop(columns=['the_geom'], inplace=True)\n",
    "        if 'the_geom_webmercator' in df.columns:\n",
    "            df.drop(columns=['the_geom_webmercator'], inplace=True)\n",
    "        df.crs = {'init': 'epsg:4326'}\n",
    "        df = df.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intersect_areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_areas(gdf, geometry):\n",
    "    \"\"\"\n",
    "    Intersection between the areas of a GeoDataFrame and a geometry\n",
    "    \"\"\"\n",
    "    sindex = gdf.sindex\n",
    "    \n",
    "    # Areas that intersect with the geometry\n",
    "    possible_matches_index = list(sindex.intersection(geometry.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    \n",
    "    # Intersection between the areas and the geometry\n",
    "    precise_matches = possible_matches.intersection(geometry)\n",
    "    \n",
    "    # Replace areas with the intersected ones\n",
    "    final_matches = possible_matches[~precise_matches.is_empty]\n",
    "    final_matches['geometry'] = list(precise_matches[~precise_matches.is_empty])\n",
    "    \n",
    "    return final_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot_hist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(x_min, count):\n",
    "    width = x_min[1]-x_min[0]\n",
    "    width -= width/5.\n",
    "    x_min += width/(5.*2)\n",
    "    per = count/count.sum()*100\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.bar(x_min, per, width=width)\n",
    "    \n",
    "    plt.plot([0,0], [0,per.max()], color = 'k', linestyle = '--')\n",
    "    \n",
    "    plt.title('Soil Organic Carbon Stock')\n",
    "    plt.xlabel('SOC stock t C/ha)')\n",
    "    plt.ylabel('(%) of total area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set_lat_lon_attrs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lat_lon_attrs(ds):\n",
    "    \"\"\" Set CF latitude and longitude attributes\"\"\"\n",
    "    ds[\"lon\"] = ds.lon.assign_attrs({\n",
    "      'axis' : 'X',\n",
    "       'long_name' : 'longitude',\n",
    "        'standard_name' : 'longitude',\n",
    "         'stored_direction' : 'increasing',\n",
    "          'type' : 'double',\n",
    "           'units' : 'degrees_east',\n",
    "            'valid_max' : 360.0,\n",
    "             'valid_min' : -180.0\n",
    "             })\n",
    "    ds[\"lat\"] = ds.lat.assign_attrs({\n",
    "      'axis' : 'Y',\n",
    "       'long_name' : 'latitude',\n",
    "        'standard_name' : 'latitude',\n",
    "         'stored_direction' : 'increasing',\n",
    "          'type' : 'double',\n",
    "           'units' : 'degrees_north',\n",
    "            'valid_max' : 90.0,\n",
    "             'valid_min' : -90.0\n",
    "             })\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create_ds_mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_mask(df, ds, id_prop, name_prop, name, debug=False):\n",
    "  \n",
    "    # Get mean ds cell area (in degrees) \n",
    "    mean_y_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_y_size)\n",
    "    mean_x_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_x_size)\n",
    "    mean_area = mean_y_size * mean_x_size\n",
    "    print(f\"The mean ds cell area is {np.round(mean_area, 6)} deg.\\n\")\n",
    "    \n",
    "    # Clip gdf to bounding box of ds\n",
    "    xmin = ds.lon.min().values.tolist()\n",
    "    xmax = ds.lon.max().values.tolist()\n",
    "    ymin = ds.lat.min().values.tolist()\n",
    "    ymax = ds.lat.max().values.tolist()\n",
    "    df = df.cx[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    \n",
    "    # Add area of geoms to gdf\n",
    "    df = df.assign(area = df.area)\n",
    "    df = df.assign(area_is_gt_cell = df['area'] > mean_area)\n",
    "    print(f\"Clipped gdf to dataset bounds, giving {len(df[id_prop])} potential geometries, of which {df['area_is_gt_cell'].sum()} are large enough.\\n\")\n",
    "    \n",
    "    print(\"Geometries smaller than mean cell size:\")\n",
    "    print(df.loc[df['area_is_gt_cell'] == False, [id_prop, name_prop]])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Extract geoms, codes, names, and create code_ints that are large enough!\n",
    "    id_names = df.loc[df['area_is_gt_cell'] == True, name_prop].values\n",
    "    if debug:\n",
    "        print(\"\\n NAMEs for mask:\\n\")\n",
    "        print(len(id_names))\n",
    "        print(id_names)\n",
    "    id_codes = df.loc[df['area_is_gt_cell'] == True, id_prop].values\n",
    "    if debug:\n",
    "        print(\"\\n CODEs for mask:\\n\")\n",
    "        print(len(id_codes))\n",
    "        print(id_codes)\n",
    "    geoms = df.loc[df['area_is_gt_cell'] == True, 'geometry'].values\n",
    "    id_ints = list(range(0, len(id_codes)))\n",
    "    if debug:\n",
    "        print(\"\\n IDs for mask:\\n\")\n",
    "        print(len(id_ints))\n",
    "        print(id_ints)\n",
    "\n",
    "    # create mask object\n",
    "    da_mask = regionmask.Regions(\n",
    "      name = name,\n",
    "      numbers = id_ints,\n",
    "      names = id_names,\n",
    "      abbrevs = id_codes,\n",
    "      outlines = geoms)\\\n",
    "      .mask(ds)\\\n",
    "      .rename(name)\n",
    "    if debug:\n",
    "        print(da_mask)\n",
    "\n",
    "    # get the ints actually written to mask\n",
    "    id_ints_mask = da_mask.to_dataframe().dropna()[name].unique()\n",
    "    id_ints_mask = np.sort(id_ints_mask).astype('int')\n",
    "    if debug:\n",
    "        print(\"\\n Actually IDs on mask:\\n\")\n",
    "        print(len(id_ints_mask))\n",
    "        print(id_ints_mask)\n",
    "    \n",
    "    # resample the ints, names and codes\n",
    "    id_names_mask = id_names[id_ints_mask]\n",
    "    if debug:\n",
    "        print(\"\\n Actually NAMES on mask:\\n\")\n",
    "        print(len(id_names_mask))\n",
    "        print(id_names_mask)\n",
    "    id_codes_mask = id_codes[id_ints_mask]\n",
    "    if debug:\n",
    "        print(\"\\n Actually CODES on mask:\\n\")\n",
    "        print(len(id_codes_mask))\n",
    "        print(id_codes_mask)\n",
    "    print(f\"Finished writing {len(id_codes_mask)} geometries to {name} mask.\\n\")\n",
    "    \n",
    "    # update da attributes\n",
    "    da_mask.attrs['id_ints'] = id_ints_mask\n",
    "    da_mask.attrs['id_codes'] = id_codes_mask  \n",
    "    da_mask.attrs['id_names'] = id_names_mask\n",
    "    da_mask = set_lat_lon_attrs(da_mask)\n",
    "    return da_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precalculate_change**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_change(df, xds, nBinds=40, bindsRange=[-50, 50]):\n",
    "    indexes = xds.index.attrs.get('id_ints').astype(np.float32)\n",
    "    times = xds.coords.get('time').values\n",
    "    depths = xds.coords.get('depth').values\n",
    "    \n",
    "    Indexes = []\n",
    "    Counts = []\n",
    "    Bins = []\n",
    "    Change = []\n",
    "    Depth = []\n",
    "    Dates = []\n",
    "    \n",
    "    for index in tqdm(indexes):\n",
    "        xds_index = xds.where(xds['index'].isin(index))\n",
    "        for depth in depths:\n",
    "            for i in range(len(times)):\n",
    "                for j in range(len(times)-1-i):\n",
    "                    start_date = times[i]\n",
    "                    end_date = times[i+j+1]\n",
    "                    \n",
    "                    # Get difference between two dates\n",
    "                    diff = xds_index.loc[dict(time=end_date, depth=depth)] - xds_index.loc[dict(time=start_date, depth=depth)]\n",
    "                    \n",
    "                    # Get counts and binds of the histogram\n",
    "                    h, bins = da.histogram(diff.stocks, bins=nBinds, range=bindsRange)\n",
    "                    \n",
    "                    # Compute change value\n",
    "                    start_year = pd.to_datetime(start_date).year\n",
    "                    end_year = pd.to_datetime(end_date).year\n",
    "                    dYears = end_year - start_year\n",
    "                    mean_diff = diff.stocks.mean(skipna=True).compute().values\n",
    "                    change = mean_diff/dYears\n",
    "                    \n",
    "                    # Save values\n",
    "                    Indexes.append(int(index))\n",
    "                    Counts.append(h.compute())\n",
    "                    Bins.append(bins)\n",
    "                    Change.append(change)\n",
    "                    Depth.append(depth)\n",
    "                    Dates.append([start_year, end_year])\n",
    "                    \n",
    "    \n",
    "    df_change = pd.DataFrame({\"index\": Indexes, \"counts\": Counts, \"bins\": Bins, \"change\":Change, \"depth\": Depth, \"years\": Dates})\n",
    "    \n",
    "    return pd.merge(df.drop(columns='geometry').reset_index(drop=True).reset_index(), \n",
    "                    df_change, \n",
    "                    how='left', \n",
    "                    on='index').drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `xarray.Dataset` from `Zarr` in Google cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Connect to GS\n",
    "project_name = 'soc-platform'\n",
    "bucket_name = 'vizz-data-transfer'\n",
    "root = bucket_name+'/SOC_maps/soil-data.zarr'\n",
    "group = 'experimental-dataset-stock'\n",
    "private_key = json.loads(os.getenv(\"PRIVATE_KEY\"))\n",
    "\n",
    "gc = gcsfs.GCSFileSystem(project=project_name, token=private_key)\n",
    "store = gc.get_mapper(root)\n",
    "\n",
    "# Return zarr group\n",
    "xds = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "\n",
    "# Change coordinates names\n",
    "xds = xds.rename({'x': 'lon', 'y': 'lat'})\n",
    "\n",
    "# Change depth coord from 0 to 1 dimensional array\n",
    "depths = xds.coords.get('depth').values\n",
    "if depths.ndim == 0: \n",
    "    xds = xds.squeeze().drop(\"depth\")\n",
    "    xds = xds.assign_coords({\"depth\": np.array([depths])})\n",
    "\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read vector data\n",
    "**Political boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_data(iso=None, tolerance=None):\n",
    "    # Read Political boundaries:\n",
    "    print('Reading Political boundaries')\n",
    "    gdf_pb = gpd.read_file('../data/mbtiles/gadm36_political_boundaries/gadm36_political_boundaries.shp')\n",
    "    # Select up to level 1 admin areas\n",
    "    gdf_pb = gdf_pb[gdf_pb['level'] <= 1]\n",
    "    \n",
    "    # Read Landforms\n",
    "    print('Reading Landforms')\n",
    "    gdf_land = gpd.read_file('../data/mbtiles/ne_10m_geography_regions_polys/ne_10m_geography_regions_polys.shp')\n",
    "    \n",
    "    # Read Biomes\n",
    "    print('Reading Biomes')\n",
    "    gdf_bio = gpd.read_file('../data/mbtiles/bio_042_ecoregions_by_biome_1_14/bio_042_ecoregions_by_biome_1_14.shp')\n",
    "    \n",
    "    # Read Hydrological basins\n",
    "    print('Reading Hydrological basins')\n",
    "    gdf_hb = gpd.read_file('../data/mbtiles/hydrological_basins/hydrological_basins.shp')\n",
    "    #Make valid hydrological basin geometries\n",
    "    gdf_hb['geometry'] = gdf_hb['geometry'].apply(lambda x: x.buffer(0))\n",
    "\n",
    "    vector_data = {'political_boundaries': gdf_pb, 'landforms': gdf_land, 'biomes': gdf_bio, 'hydrological_basins': gdf_hb}\n",
    "    \n",
    "    if iso:\n",
    "        print('Intersecting areas with the selected country')\n",
    "        gdf_pb = gdf_pb[gdf_pb['gid_0'] == iso]\n",
    "        if tolerance:\n",
    "            gdf_pb['geometry'] = gdf_pb['geometry'].apply(lambda x: x.simplify(tolerance)) \n",
    "            \n",
    "        vector_data['political_boundaries'] = gdf_pb\n",
    "        \n",
    "        country = gdf_pb[gdf_pb['level'] == 0]['geometry'].iloc[0].buffer(0)\n",
    "        \n",
    "        for data_name in list(vector_data.keys())[1:]:\n",
    "            print(data_name)\n",
    "            vector_data[data_name] = intersect_areas(vector_data[data_name], country)\n",
    "            \n",
    "    # Set index\n",
    "    for data_name in list(vector_data.keys()):\n",
    "        vector_data[data_name] = vector_data[data_name].reset_index(drop=True).reset_index()\n",
    "        \n",
    "    return vector_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data = prepare_vector_data(iso='ARG', tolerance=0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "query = \"SELECT name_0, name_1, gid_0, gid_1, the_geom FROM gadm36_political_boundaries WHERE level = 1 and gid_0 = 'ARG'\"\n",
    "\n",
    "df = df_from_carto(account, query)\n",
    "\n",
    "df = df.iloc[10:12]\n",
    "#df['geometry'] = df['geometry'].apply(lambda x: x.simplify(0.05))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8.5))\n",
    "df.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the data mask by rasterizing the vector data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_mask_2(df, ds, name):\n",
    "    \n",
    "    # Create index column\n",
    "    if 'index' not in df:\n",
    "        df.reset_index(drop=True).reset_index()\n",
    "    \n",
    "    # Get mean ds cell area (in degrees) \n",
    "    mean_y_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_y_size)\n",
    "    mean_x_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_x_size)\n",
    "    mean_area = mean_y_size * mean_x_size\n",
    "    print(f\"The mean ds cell area is {np.round(mean_area, 6)} deg.\\n\")\n",
    "    \n",
    "    # Clip gdf to bounding box of ds\n",
    "    xmin = ds.lon.min().values.tolist()\n",
    "    xmax = ds.lon.max().values.tolist()\n",
    "    ymin = ds.lat.min().values.tolist()\n",
    "    ymax = ds.lat.max().values.tolist()\n",
    "    df = df.cx[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    \n",
    "    # Add area of geoms to gdf\n",
    "    df = df.assign(area = df.area)\n",
    "    df = df.assign(area_is_gt_cell = df['area'] > mean_area)\n",
    "    print(f\"Clipped gdf to dataset bounds, giving {len(df['index'])} potential geometries, of which {df['area_is_gt_cell'].sum()} are large enough.\\n\")\n",
    "    \n",
    "    print(\"Geometries smaller than mean cell size:\")\n",
    "    print(df.loc[df['area_is_gt_cell'] == False, ['index']])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Extract indexes and geoms that are large enough!\n",
    "    id_ints = df.loc[df['area_is_gt_cell'] == True, 'index'].values\n",
    "    geoms = df.loc[df['area_is_gt_cell'] == True, 'geometry'].values\n",
    "    \n",
    "    print(f'Number of indexes: {len(id_ints)}')\n",
    "    print(f'Number of geoms: {len(geoms)}')\n",
    "\n",
    "    # create mask object\n",
    "    da_mask = regionmask.Regions(\n",
    "      name = name,\n",
    "      numbers = id_ints,\n",
    "      outlines = geoms)\\\n",
    "      .mask(ds)\\\n",
    "      .rename(name)\n",
    "\n",
    "    # get the ints actually written to mask\n",
    "    id_ints_mask = da_mask.to_dataframe().dropna()[name].unique()\n",
    "    id_ints_mask = np.sort(id_ints_mask).astype('int')\n",
    "    \n",
    "    print(f'Number of ints ints mask: {len(id_ints_mask)}')\n",
    "    \n",
    "    # update da attributes\n",
    "    da_mask.attrs['id_ints'] = id_ints_mask\n",
    "    da_mask = set_lat_lon_attrs(da_mask)\n",
    "    return da_mask, id_ints, geoms, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='landforms'\n",
    "da_mask, id_ints, geoms, df = create_ds_mask_2(vector_data[name], xds, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mask.to_dataframe().dropna()[name].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms[5].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mask = regionmask.Regions(\n",
    "      name = name,\n",
    "      numbers = id_ints[:4],\n",
    "      outlines = geoms[:4])\\\n",
    "      .mask(xds)\\\n",
    "      .rename(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = regionmask.Regions(\n",
    "    name = name, \n",
    "    numbers = id_ints[:3], \n",
    "    outlines = geoms[:3].buffer(5))\n",
    "da_mask = regions.mask(xds, wrap_lon=False).rename(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mask.to_dataframe().dropna()[name].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xds.copy()\n",
    "test[\"index\"] = da_mask\n",
    "test = test.where(test.index == 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "f, ax = plt.subplots(1, 1, subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "ax.add_geometries(regions.polygons, ccrs.PlateCarree(), fc=\"none\", ec=\"0.1\")\n",
    "test.isel(time=1).stocks.plot.pcolormesh(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_masks = {}\n",
    "for name in vector_data.keys():\n",
    "    print(f'Creating xarray DataArray mask for {name}:')\n",
    "    da_masks[name] = create_ds_mask_2(vector_data[name], xds, name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_masks['landforms'].to_dataframe().dropna()['landforms'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data['landforms']['geometry'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = 'Argentina'\n",
    "name_prop = 'name_1'\n",
    "id_prop = 'gid_1'\n",
    "\n",
    "da_mask = create_ds_mask(df, xds, id_prop, name_prop, name, debug=False)\n",
    "da_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8.5))\n",
    "ax.imshow(da_mask.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `xarray.Dataset` by merging both `xarray.Datasets`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds[\"index\"] = da_mask\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "#### **Example**\n",
    "Select subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds = xds.isel(time=[0,18,35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = xds.index.attrs.get('id_ints').astype(np.float32)\n",
    "times = xds.coords.get('time').values\n",
    "depths = xds.coords.get('depth').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds_index = xds.where(xds['index'].isin(indexes[0]))\n",
    "diff = xds_index.loc[dict(time=times[2], depth=depths[0])] - xds_index.loc[dict(time=times[0], depth=depths[0])]\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with `xhistogram`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-50, 50, 41)\n",
    "h = histogram(diff.stocks, bins=[bins], dim=['lat', 'lon'])\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    h.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot change distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    count = h.values\n",
    "    \n",
    "x_min = bins[:-1]\n",
    "plot_hist(x_min, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with `da.histogram`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, bins = da.histogram(diff.stocks, bins=40, range=[-50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot change distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    count = h.compute()\n",
    "\n",
    "x_min = bins[:-1]\n",
    "plot_hist(x_min, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display change value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.to_datetime(times[2]).year - pd.to_datetime(times[0]).year\n",
    "mean_diff = diff.stocks.mean(skipna=True).compute().values\n",
    "change = mean_diff/years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Soil Organic Carbon Stock Change: {change} t C/ha year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entire` DataFrame`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ProgressBar():\n",
    "    result = precalculate_change(df, xds, nBinds=40, bindsRange=[-50, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot change distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "count = result['counts'].iloc[index].copy()\n",
    "bins = result['bins'].iloc[index].copy()\n",
    "change = result['change'].iloc[index].copy()\n",
    "x_min = bins[:-1]\n",
    "per = count/count.sum()*100\n",
    "print(f'Soil Organic Carbon Stock Change: {change} t C/ha year')\n",
    "plot_hist(x_min, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean value\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_xds = xds.groupby(xds.index)\n",
    "grid_mean = grouped_xds.mean().rename({\"stocks\": \"mean\"})\n",
    "grid_mean.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_from_latlon(lat, lon):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    trans = Affine.translation(lon[0], lat[0])\n",
    "    scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "    return trans * scale\n",
    "\n",
    "def rasterize(shapes, coords, latitude='latitude', longitude='longitude',\n",
    "              fill=np.nan, **kwargs):\n",
    "    \"\"\"Rasterize a list of (geometry, fill_value) tuples onto the given\n",
    "    xray coordinates. This only works for 1d latitude and longitude\n",
    "    arrays.\n",
    "    \"\"\"\n",
    "    transform = transform_from_latlon(coords[latitude], coords[longitude])\n",
    "    out_shape = (len(coords[latitude]), len(coords[longitude]))\n",
    "    raster = features.rasterize(shapes, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=float, **kwargs)\n",
    "    spatial_coords = {latitude: coords[latitude], longitude: coords[longitude]}\n",
    "    return xr.DataArray(raster, coords=spatial_coords, dims=(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_pb = gpd.read_file('../data/mbtiles/gadm36_political_boundaries/gadm36_political_boundaries.shp')\n",
    "# Select up to level 1 admin areas\n",
    "gdf_pb = gdf_pb[gdf_pb['level'] == 1]\n",
    "gdf_pb = gdf_pb[gdf_pb['gid_0'] == 'ARG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_pb = gdf_pb.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ids = {k: i for i, k in enumerate(gdf_pb.name_1)}\n",
    "state_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = zip(gdf_pb.geometry.buffer(0), range(len(gdf_pb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xds.copy()\n",
    "ds['states'] = rasterize(shapes, ds.coords, longitude='lon', latitude='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.states.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.states == state_ids['Corrientes']).plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.stocks\n",
    " .isel(time=slice(12))\n",
    " .where(ds.states == state_ids['Corrientes'])\n",
    " .sel(lon=slice(-62, -55), lat=slice(-32, -25))\n",
    " .plot.imshow(col='time', col_wrap=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
