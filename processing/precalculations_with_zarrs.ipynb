{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soils Revealed precalculations with `Zarrs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>\n",
    "### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler, visualize\n",
    "from xhistogram.xarray import histogram\n",
    "from rasterio import features\n",
    "import zarr\n",
    "import rioxarray\n",
    "import regionmask\n",
    "import gcsfs\n",
    "import s3fs\n",
    "from geocube.api.core import make_geocube\n",
    "import shapely.wkb \n",
    "from shapely.ops import cascaded_union\n",
    "import json\n",
    "import requests\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path \n",
    "env_path = Path('.') / '.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://scheduler:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://scheduler:8787/status' target='_blank'>http://scheduler:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>48</li>\n",
       "  <li><b>Memory: </b>135.02 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.19.0.4:8786' processes=1 threads=48, memory=135.02 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "#cluster = LocalCluster(n_workers=1, threads_per_worker=36)\n",
    "client = Client()  # start distributed scheduler locally.  Launch dashboard\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='utils'></a>\n",
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intersect_areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_areas(gdf, geometry):\n",
    "    \"\"\"\n",
    "    Intersection between the areas of a GeoDataFrame and a geometry\n",
    "    \"\"\"\n",
    "    sindex = gdf.sindex\n",
    "    \n",
    "    # Areas that intersect with the geometry\n",
    "    possible_matches_index = list(sindex.intersection(geometry.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    \n",
    "    # Intersection between the areas and the geometry\n",
    "    precise_matches = possible_matches.intersection(geometry)\n",
    "    \n",
    "    # Replace areas with the intersected ones\n",
    "    final_matches = possible_matches[~precise_matches.is_empty]\n",
    "    final_matches['geometry'] = list(precise_matches[~precise_matches.is_empty])\n",
    "    \n",
    "    return final_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot_hist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(x_min, count):\n",
    "    width = x_min[1]-x_min[0]\n",
    "    width -= width/5.\n",
    "    x_min += width/(5.*2)\n",
    "    per = count/count.sum()*100\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.bar(x_min, per, width=width)\n",
    "    \n",
    "    plt.plot([0,0], [0,per.max()], color = 'k', linestyle = '--')\n",
    "    \n",
    "    plt.title('Soil Organic Carbon Stock')\n",
    "    plt.xlabel('SOC stock t C/ha)')\n",
    "    plt.ylabel('(%) of total area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read_dataset_from_zarr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_from_zarr(s3_path, group):\n",
    "    # Initilize the S3 file system\n",
    "    s3 = s3fs.S3FileSystem(key=os.getenv(\"S3_ACCESS_KEY_ID\"), secret=os.getenv(\"S3_SECRET_ACCESS_KEY\"))\n",
    "    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "    # Read Zarr file\n",
    "    ds = xr.open_zarr(store=store, group=group, consolidated=True)\n",
    "    \n",
    "    # Change coordinates names\n",
    "    ds = ds.rename({'x': 'lon', 'y': 'lat'})\n",
    "    \n",
    "    # Change dimension name\n",
    "    if group == 'concentration':\n",
    "        ds = ds.rename({'depht': 'depth'})\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read_dataset_from_zarr_local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_from_zarr_local(path, group):\n",
    "    # Read Zarr file\n",
    "    ds = xr.open_zarr(store=path, group=group, consolidated=True)\n",
    "    \n",
    "    # Change coordinates names\n",
    "    ds = ds.rename({'x': 'lon', 'y': 'lat'})\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare_vector_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_data(iso=None, tolerance=None):\n",
    "    if iso:\n",
    "        bboxs = pd.read_csv('../data/mbtiles/country_bbox.csv', converters={\"bbox\": literal_eval})\n",
    "        bbox = bboxs[bboxs['gid_0'] == iso].bbox.iloc[0]\n",
    "    else:\n",
    "        bbox = None\n",
    "        \n",
    "    # Read Political boundaries:\n",
    "    print('Reading Political boundaries')\n",
    "    gdf_pb = gpd.read_file('../data/mbtiles/political_boundaries/political_boundaries.shp', bbox = bbox)\n",
    "    # Select up to level 1 admin areas\n",
    "    gdf_pb = gdf_pb[gdf_pb['level'] <= 1]\n",
    "    #Simplify geometries\n",
    "    if tolerance:\n",
    "        gdf_pb['geometry'] = gdf_pb['geometry'].apply(lambda x: x.simplify(tolerance)) \n",
    "    # Add area in ha\n",
    "    gdf_pb['area_ha'] = gdf_pb['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4)    \n",
    "    gdf_pb = gdf_pb[['name_0', 'gid_0', 'name_1', 'gid_1', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "    # Read Landforms\n",
    "    print('Reading Landforms')\n",
    "    gdf_land = gpd.read_file('../data/mbtiles/ne_10m_geography_regions/ne_10m_geography_regions.shp', bbox = bbox)\n",
    "    # Add area in ha\n",
    "    gdf_land = gdf_land.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf_land = gdf_land.to_crs(\"EPSG:4326\")\n",
    "    gdf_land['area_ha'] = gdf_land['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4)   \n",
    "    gdf_land = gdf_land[['featurecla', 'name', 'region', 'ne_id', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "    # Read Biomes\n",
    "    print('Reading Biomes')\n",
    "    gdf_bio = gpd.read_file('../data/mbtiles/ecoregions_by_biome/ecoregions_by_biome.shp', bbox = bbox)\n",
    "    # Add area in ha\n",
    "    gdf_bio = gdf_bio.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf_bio = gdf_bio.to_crs(\"EPSG:4326\")\n",
    "    gdf_bio['area_ha'] = gdf_bio['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4) \n",
    "    gdf_bio = gdf_bio[['biome_name', 'biome_num', 'eco_name', 'eco_biome_', 'eco_id', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "    # Read Hydrological basins\n",
    "    print('Reading Hydrological basins')\n",
    "    gdf_hb = gpd.read_file('../data/mbtiles/hydrological_basins/hydrological_basins.shp', bbox = bbox)\n",
    "    #Make valid geometries\n",
    "    gdf_hb['geometry'] = gdf_hb['geometry'].apply(lambda x: x.buffer(0))\n",
    "    # Add area in ha\n",
    "    gdf_hb = gdf_hb.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf_hb = gdf_hb.to_crs(\"EPSG:4326\")\n",
    "    gdf_hb['area_ha'] = gdf_hb['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4) \n",
    "    gdf_hb = gdf_hb[['maj_bas', 'maj_name', 'maj_area', 'sub_bas', 'sub_name', 'sub_area', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "\n",
    "    vector_data = {'political_boundaries': gdf_pb, 'landforms': gdf_land, 'biomes': gdf_bio, 'hydrological_basins': gdf_hb}\n",
    "    \n",
    "    if iso:\n",
    "        print('Intersecting areas with the selected country')\n",
    "        gdf_pb = gdf_pb[gdf_pb['gid_0'] == iso]\n",
    "            \n",
    "        vector_data['political_boundaries'] = gdf_pb\n",
    "        \n",
    "        country = gdf_pb[gdf_pb['level'] == 0]['geometry'].iloc[0].buffer(0)\n",
    "        \n",
    "        for data_name in list(vector_data.keys())[1:]:\n",
    "            print(data_name)\n",
    "            vector_data[data_name] = intersect_areas(vector_data[data_name], country)\n",
    "            \n",
    "    # Split DataFrames to avoid overlapping geometries\n",
    "    split_df_by = ['level', 'level', 'level', 'level']\n",
    "    print(\"splitting DataFrames\")\n",
    "    for n, data_name in enumerate(list(vector_data.keys())):\n",
    "        if split_df_by[n]:\n",
    "            df = vector_data[data_name].copy()\n",
    "            del vector_data[data_name]\n",
    "            categories = list(df[split_df_by[n]].unique())\n",
    "            for category in categories:\n",
    "                vector_data[data_name+'_'+str(category)] = df[df[split_df_by[n]] == category]    \n",
    "            \n",
    "    # Further split DataFrames landforms_1\n",
    "    df = vector_data['landforms_1'].copy()\n",
    "    del vector_data['landforms_1']\n",
    "    categories = list(df['featurecla'].unique())\n",
    "    for category in categories:\n",
    "        vector_data['landforms_1'+'_'+str(category)] = df[df['featurecla'] == category] \n",
    "                \n",
    "    # Set index\n",
    "    for data_name in list(vector_data.keys()):\n",
    "        vector_data[data_name] = vector_data[data_name].reset_index(drop=True).reset_index()\n",
    "   \n",
    "    return vector_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare_vector_data_political_boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_data_political_boundaries(iso=None, tolerance=None):\n",
    "    if iso:\n",
    "        bboxs = pd.read_csv('../data/mbtiles/country_bbox.csv', converters={\"bbox\": literal_eval})\n",
    "        bbox = bboxs[bboxs['gid_0'] == iso].bbox.iloc[0]\n",
    "    else:\n",
    "        bbox = None\n",
    "        \n",
    "    # Read Political boundaries:\n",
    "    print('Reading Political boundaries')\n",
    "    gdf_pb = gpd.read_file('../data/mbtiles/political_boundaries/political_boundaries.shp', bbox = bbox)\n",
    "    # Select up to level 1 admin areas\n",
    "    gdf_pb = gdf_pb[gdf_pb['level'] <= 1]\n",
    "    #Simplify geometries\n",
    "    if tolerance:\n",
    "        gdf_pb['geometry'] = gdf_pb['geometry'].apply(lambda x: x.simplify(tolerance)) \n",
    "    # Add area in ha\n",
    "    gdf_pb['area_ha'] = gdf_pb['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4)    \n",
    "    gdf_pb = gdf_pb[['name_0', 'gid_0', 'name_1', 'gid_1', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "\n",
    "    vector_data = {'political_boundaries': gdf_pb}\n",
    "    \n",
    "    if iso:\n",
    "        print('Intersecting areas with the selected country')\n",
    "        gdf_pb = gdf_pb[gdf_pb['gid_0'] == iso]\n",
    "            \n",
    "        vector_data['political_boundaries'] = gdf_pb\n",
    "        \n",
    "        country = gdf_pb[gdf_pb['level'] == 0]['geometry'].iloc[0].buffer(0)\n",
    "        \n",
    "            \n",
    "    # Split DataFrames to avoid overlapping geometries\n",
    "    split_df_by = ['level', 'level', 'level', 'level']\n",
    "    print(\"splitting DataFrames\")\n",
    "    for n, data_name in enumerate(list(vector_data.keys())):\n",
    "        if split_df_by[n]:\n",
    "            df = vector_data[data_name].copy()\n",
    "            del vector_data[data_name]\n",
    "            categories = list(df[split_df_by[n]].unique())\n",
    "            for category in categories:\n",
    "                vector_data[data_name+'_'+str(category)] = df[df[split_df_by[n]] == category]    \n",
    "\n",
    "    # Set index\n",
    "    for data_name in list(vector_data.keys()):\n",
    "        vector_data[data_name] = vector_data[data_name].reset_index(drop=True).reset_index()\n",
    "   \n",
    "    return vector_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare_vector_data_level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_data_level(iso=None, tolerance=None, level=1):\n",
    "    if iso:\n",
    "        bboxs = pd.read_csv('../data/mbtiles/country_bbox.csv', converters={\"bbox\": literal_eval})\n",
    "        bbox = bboxs[bboxs['gid_0'] == iso].bbox.iloc[0]\n",
    "    else:\n",
    "        bbox = None\n",
    "        \n",
    "    # Read Political boundaries:\n",
    "    print('Reading Political boundaries')\n",
    "    gdf_pb = gpd.read_file('../data/mbtiles/political_boundaries/political_boundaries.shp', bbox = bbox)\n",
    "    # Select level 0 areas\n",
    "    gdf_pb_0 = gdf_pb[gdf_pb['level'] == 0]\n",
    "    # Select level 1 areas\n",
    "    gdf_pb = gdf_pb[gdf_pb['level'] == level]\n",
    "    #Simplify geometries\n",
    "    if tolerance:\n",
    "        gdf_pb['geometry'] = gdf_pb['geometry'].apply(lambda x: x.simplify(tolerance)) \n",
    "    # Add area in ha\n",
    "    gdf_pb['area_ha'] = gdf_pb['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4)    \n",
    "    gdf_pb = gdf_pb[['name_0', 'gid_0', 'name_1', 'gid_1', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "    # Read Landforms\n",
    "    print('Reading Landforms')\n",
    "    gdf_land = gpd.read_file('../data/mbtiles/ne_10m_geography_regions/ne_10m_geography_regions.shp', bbox = bbox)\n",
    "    # Select level 1 areas\n",
    "    gdf_land = gdf_land[gdf_land['level'] == level]\n",
    "    # Add area in ha\n",
    "    gdf_land = gdf_land.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf_land = gdf_land.to_crs(\"EPSG:4326\")\n",
    "    gdf_land['area_ha'] = gdf_land['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4)   \n",
    "    gdf_land = gdf_land[['featurecla', 'name', 'region', 'ne_id', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "    # Read Biomes\n",
    "    print('Reading Biomes')\n",
    "    gdf_bio = gpd.read_file('../data/mbtiles/ecoregions_by_biome/ecoregions_by_biome.shp', bbox = bbox)\n",
    "    # Select level 1 areas\n",
    "    gdf_bio = gdf_bio[gdf_bio['level'] == level]    \n",
    "    # Add area in ha\n",
    "    gdf_bio = gdf_bio.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf_bio = gdf_bio.to_crs(\"EPSG:4326\")\n",
    "    gdf_bio['area_ha'] = gdf_bio['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4) \n",
    "    gdf_bio = gdf_bio[['biome_name', 'biome_num', 'eco_name', 'eco_biome_', 'eco_id', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "    \n",
    "    # Read Hydrological basins\n",
    "    print('Reading Hydrological basins')\n",
    "    gdf_hb = gpd.read_file('../data/mbtiles/hydrological_basins/hydrological_basins.shp', bbox = bbox)\n",
    "    # Select level 1 areas\n",
    "    gdf_hb = gdf_hb[gdf_hb['level'] == level]    \n",
    "    #Make valid geometries\n",
    "    gdf_hb['geometry'] = gdf_hb['geometry'].apply(lambda x: x.buffer(0))\n",
    "    # Add area in ha\n",
    "    gdf_hb = gdf_hb.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf_hb = gdf_hb.to_crs(\"EPSG:4326\")\n",
    "    gdf_hb['area_ha'] = gdf_hb['geometry'].to_crs({'init': 'epsg:6933'}).map(lambda p: p.area / 10**4) \n",
    "    gdf_hb = gdf_hb[['maj_bas', 'maj_name', 'maj_area', 'sub_bas', 'sub_name', 'sub_area', 'level', 'bbox', 'area_ha', 'id', 'id_0', 'geometry']]\n",
    "\n",
    "    vector_data = {f'political_boundaries_{str(level)}': gdf_pb, f'landforms_{str(level)}': gdf_land, f'biomes_{str(level)}': gdf_bio, f'hydrological_basins_{str(level)}': gdf_hb}\n",
    "    \n",
    "    if iso:\n",
    "        print('Intersecting areas with the selected country')\n",
    "        gdf_pb = gdf_pb[gdf_pb['gid_0'] == iso]\n",
    "            \n",
    "        vector_data[f'political_boundaries_{str(level)}'] = gdf_pb\n",
    "        \n",
    "        gdf_pb_0 = gdf_pb_0[gdf_pb_0['gid_0'] == iso]\n",
    "        country = gdf_pb_0[gdf_pb_0['level'] == 0]['geometry'].iloc[0].buffer(0)\n",
    "        \n",
    "        for data_name in list(vector_data.keys())[1:]:\n",
    "            print(data_name)\n",
    "            vector_data[data_name] = intersect_areas(vector_data[data_name], country)\n",
    "                    \n",
    "    # Set index\n",
    "    for data_name in list(vector_data.keys()):\n",
    "        vector_data[data_name] = vector_data[data_name].reset_index(drop=True).reset_index()\n",
    "   \n",
    "    return vector_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set_lat_lon_attrs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lat_lon_attrs(ds):\n",
    "    \"\"\" Set CF latitude and longitude attributes\"\"\"\n",
    "    ds[\"lon\"] = ds.lon.assign_attrs({\n",
    "      'axis' : 'X',\n",
    "       'long_name' : 'longitude',\n",
    "        'standard_name' : 'longitude',\n",
    "         'stored_direction' : 'increasing',\n",
    "          'type' : 'double',\n",
    "           'units' : 'degrees_east',\n",
    "            'valid_max' : 360.0,\n",
    "             'valid_min' : -180.0\n",
    "             })\n",
    "    ds[\"lat\"] = ds.lat.assign_attrs({\n",
    "      'axis' : 'Y',\n",
    "       'long_name' : 'latitude',\n",
    "        'standard_name' : 'latitude',\n",
    "         'stored_direction' : 'increasing',\n",
    "          'type' : 'double',\n",
    "           'units' : 'degrees_north',\n",
    "            'valid_max' : 90.0,\n",
    "             'valid_min' : -90.0\n",
    "             })\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create_ds_mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_mask(df, ds, name, lon_name='lon', lat_name='lat'):\n",
    "    # Create index column\n",
    "    if 'index' not in df:\n",
    "        df.reset_index(drop=True).reset_index()\n",
    "    \n",
    "    # Get mean ds cell area (in degrees) \n",
    "    mean_y_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_y_size)\n",
    "    mean_x_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_x_size)\n",
    "    mean_area = mean_y_size * mean_x_size\n",
    "    print(f\"The mean ds cell area is {np.round(mean_area, 6)} deg.\\n\")\n",
    "    \n",
    "    # Clip gdf to bounding box of ds\n",
    "    xmin = ds.lon.min().values.tolist()\n",
    "    xmax = ds.lon.max().values.tolist()\n",
    "    ymin = ds.lat.min().values.tolist()\n",
    "    ymax = ds.lat.max().values.tolist()\n",
    "    df = df.cx[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    \n",
    "    # Add area of geoms to gdf\n",
    "    df = df.assign(area = df.area)\n",
    "    df = df.assign(area_is_gt_cell = df['area'] > mean_area)\n",
    "    print(f\"Clipped gdf to dataset bounds, giving {len(df['index'])} potential geometries, of which {df['area_is_gt_cell'].sum()} are large enough.\\n\")\n",
    "    \n",
    "    print(\"Geometries smaller than mean cell size:\")\n",
    "    print(df.loc[df['area_is_gt_cell'] == False, ['index']])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Extract indexes and geoms that are large enough!\n",
    "    id_ints = df.loc[df['area_is_gt_cell'] == True, 'index'].values\n",
    "    geoms = df.loc[df['area_is_gt_cell'] == True, 'geometry'].values\n",
    "    \n",
    "    print(f'Number of indexes: {len(id_ints)}')\n",
    "    print(f'Number of geoms: {len(geoms)}')\n",
    "\n",
    "    # create mask object\n",
    "    da_mask = regionmask.Regions(\n",
    "      name = name,\n",
    "      numbers = id_ints,\n",
    "      outlines = geoms)\\\n",
    "      .mask(ds, lon_name=lon_name, lat_name=lat_name)\\\n",
    "      .rename(name)\n",
    "\n",
    "    # get the ints actually written to mask\n",
    "    id_ints_mask = da_mask.to_dataframe().dropna()[name].unique()\n",
    "    id_ints_mask = np.sort(id_ints_mask).astype('int')\n",
    "    \n",
    "    print(f'Number of ints in mask: {len(id_ints_mask)}')\n",
    "    \n",
    "    # update da attributes\n",
    "    da_mask.attrs['id_ints'] = id_ints_mask\n",
    "    da_mask = set_lat_lon_attrs(da_mask)\n",
    "    return da_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create_ds_mask_by_geom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_mask_by_geom(df, ds, name, lon_name='lon', lat_name='lat'):\n",
    "    # Extract indexes and geoms that are large enough!\n",
    "    id_int = df['index'].values\n",
    "    geom = df['geometry'].values\n",
    "\n",
    "    # create mask object\n",
    "    da_mask = regionmask.Regions(\n",
    "      name = name,\n",
    "      numbers = id_int,\n",
    "      outlines = geom)\\\n",
    "      .mask(ds, lon_name=lon_name, lat_name=lat_name)\\\n",
    "      .rename(name)\n",
    "    \n",
    "    # update da attributes\n",
    "    da_mask.attrs['id_ints'] = id_int\n",
    "    da_mask = set_lat_lon_attrs(da_mask)\n",
    "    return da_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`rasterio` and `geopandas` can be combined with `xarray` to make converting shapefiles into raster masks pretty easy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_from_latlon(lat, lon):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    trans = Affine.translation(lon[0], lat[0])\n",
    "    scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "    return trans * scale\n",
    "\n",
    "def rasterize(shapes, coords, latitude='latitude', longitude='longitude',\n",
    "              fill=np.nan, **kwargs):\n",
    "    \"\"\"Rasterize a list of (geometry, fill_value) tuples onto the given\n",
    "    xray coordinates. This only works for 1d latitude and longitude\n",
    "    arrays.\n",
    "    \"\"\"\n",
    "    transform = transform_from_latlon(coords[latitude], coords[longitude])\n",
    "    out_shape = (len(coords[latitude]), len(coords[longitude]))\n",
    "    raster = features.rasterize(shapes, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=float, **kwargs)\n",
    "    spatial_coords = {latitude: coords[latitude], longitude: coords[longitude]}\n",
    "    return xr.DataArray(raster, coords=spatial_coords, dims=(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_mask_rasterio(df, ds, name, lon_name='lon', lat_name='lat'):\n",
    "    # Create index column\n",
    "    if 'index' not in df:\n",
    "        df.reset_index(drop=True).reset_index()\n",
    "    \n",
    "    # Get mean ds cell area (in degrees) \n",
    "    mean_y_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_y_size)\n",
    "    mean_x_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_x_size)\n",
    "    mean_area = mean_y_size * mean_x_size\n",
    "    print(f\"The mean ds cell area is {np.round(mean_area, 6)} deg.\\n\")\n",
    "    \n",
    "    # Clip gdf to bounding box of ds\n",
    "    xmin = ds.lon.min().values.tolist()\n",
    "    xmax = ds.lon.max().values.tolist()\n",
    "    ymin = ds.lat.min().values.tolist()\n",
    "    ymax = ds.lat.max().values.tolist()\n",
    "    df = df.cx[xmin:xmax, ymin:ymax]\n",
    "\n",
    "    # Add area of geoms to gdf\n",
    "    df = df.assign(area = df.area)\n",
    "    df = df.assign(area_is_gt_cell = df['area'] > mean_area)\n",
    "    print(f\"Clipped gdf to dataset bounds, giving {len(df['index'])} potential geometries, of which {df['area_is_gt_cell'].sum()} are large enough.\\n\")\n",
    "    \n",
    "    print(\"Geometries smaller than mean cell size:\")\n",
    "    print(df.loc[df['area_is_gt_cell'] == False, ['index']])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Extract indexes and geoms that are large enough!\n",
    "    id_ints = df.loc[df['area_is_gt_cell'] == True, 'index'].values\n",
    "    geoms = df.loc[df['area_is_gt_cell'] == True, 'geometry'].values\n",
    "    \n",
    "    print(f'Number of indexes: {len(id_ints)}')\n",
    "    print(f'Number of geoms: {len(geoms)}')\n",
    "    \n",
    "    # create mask object\n",
    "    shapes = zip(df.geometry.buffer(0), range(len(df)))\n",
    "    da_mask = rasterize(shapes, ds.coords, longitude=lon_name, latitude=lat_name).rename(name)\n",
    "\n",
    "    # get the ints actually written to mask\n",
    "    id_ints_mask = da_mask.to_dataframe().dropna()[name].unique()\n",
    "    id_ints_mask = np.sort(id_ints_mask).astype('int')\n",
    "    \n",
    "    print(f'Number of ints in mask: {len(id_ints_mask)}')\n",
    "    \n",
    "    # update da attributes\n",
    "    da_mask.attrs['id_ints'] = id_ints_mask\n",
    "    da_mask = set_lat_lon_attrs(da_mask)\n",
    "    return da_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precalculate_change**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_change(df, xds, name, variable, group_type, nBinds=[40], bindsRange=[[-50, 50]]):\n",
    "    indexes = xds[name].attrs.get('id_ints').astype(np.float32)\n",
    "    times = xds.coords.get('time').values\n",
    "    depths = xds.coords.get('depth').values\n",
    "    \n",
    "    Indexes = []\n",
    "    Counts = []\n",
    "    Bins = []\n",
    "    Mean = []\n",
    "    Depth = []\n",
    "    Dates = []\n",
    "    \n",
    "    for index in tqdm(indexes):\n",
    "        xmin, ymax, xmax, ymin = df.iloc[int(index)]['geometry'].bounds\n",
    "        xds_index = xds.where(xds[name].isin(index)).sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax))\n",
    "        for n, depth in enumerate(depths):\n",
    "            try:\n",
    "                start_date = times[0]\n",
    "                end_date = times[-1]\n",
    "                \n",
    "                # Get difference between two dates\n",
    "                diff = xds_index.loc[dict(time=end_date, depth=depth)] - xds_index.loc[dict(time=start_date, depth=depth)]\n",
    "                \n",
    "                # Get counts and binds of the histogram\n",
    "                if variable == 'concentration':\n",
    "                    diff = diff[variable]/10.\n",
    "                else:\n",
    "                    diff = diff[variable]\n",
    "                \n",
    "                if len(depths) == len(nBinds):\n",
    "                    h, bins = da.histogram(diff, bins=nBinds[n], range=bindsRange[n])\n",
    "                else:\n",
    "                    h, bins = da.histogram(diff, bins=nBinds[0], range=bindsRange[0])\n",
    "                \n",
    "                # Compute change value\n",
    "                if group_type == 'historic':\n",
    "                    start_year = start_date\n",
    "                    end_year = end_date\n",
    "                else:\n",
    "                    start_year = pd.to_datetime(start_date).year\n",
    "                    end_year = pd.to_datetime(end_date).year\n",
    "                    \n",
    "                mean_diff = diff.mean(skipna=True).values   \n",
    "                \n",
    "                # Save values\n",
    "                Indexes.append(int(index))\n",
    "                Counts.append(h.compute())\n",
    "                Bins.append(bins)\n",
    "                Mean.append(mean_diff)\n",
    "                Depth.append(depth)\n",
    "                Dates.append([start_year, end_year])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    df_change = pd.DataFrame({\"index\": Indexes, \"counts\": Counts, \"bins\": Bins, \"mean_diff\":Mean, \"depth\": Depth, \"years\": Dates})\n",
    "                  \n",
    "    return pd.merge(df.drop(columns='geometry'), \n",
    "                    df_change, \n",
    "                    how='left', \n",
    "                    on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precalculate_change_by_geom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_change_by_geom(df, xds, name, variable, group_type, nBinds=[40], bindsRange=[[-50, 50]]):\n",
    "    indexes = list(df['index'])\n",
    "    times = xds.coords.get('time').values\n",
    "    depths = xds.coords.get('depth').values\n",
    "    \n",
    "    Indexes = []\n",
    "    Counts = []\n",
    "    Bins = []\n",
    "    Sum = []\n",
    "    Count = []\n",
    "    Mean = []\n",
    "    Depth = []\n",
    "    Dates = []\n",
    "    \n",
    "    for index in indexes:\n",
    "        xmin, ymax, xmax, ymin = df.iloc[index]['geometry'].bounds\n",
    "        xds_index = xds.sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax)).copy()\n",
    "        try:\n",
    "            # Rasterize geometry\n",
    "            ds_mask = create_ds_mask_by_geom(df.iloc[index:index+1], xds_index, name='mask', lon_name='lon', lat_name='lat')\n",
    "            xds_index['mask'] = ds_mask\n",
    "        except:\n",
    "            pass        \n",
    "        for n, depth in enumerate(depths):\n",
    "            try:\n",
    "                start_date = times[0]\n",
    "                end_date = times[-1]\n",
    "                \n",
    "                # Get difference between two dates\n",
    "                diff = xds_index.where(xds_index['mask'].isin(index)).loc[dict(time=end_date, depth=depth)] - xds_index.loc[dict(time=start_date, depth=depth)]\n",
    "                \n",
    "                # Get counts and binds of the histogram\n",
    "                if variable == 'concentration':\n",
    "                    diff = diff[variable]/10.\n",
    "                else:\n",
    "                    diff = diff[variable]\n",
    "                \n",
    "                if len(depths) == len(nBinds):\n",
    "                    h, bins = da.histogram(diff, bins=nBinds[n], range=bindsRange[n])\n",
    "                else:\n",
    "                    h, bins = da.histogram(diff, bins=nBinds[0], range=bindsRange[0])\n",
    "                \n",
    "                # Compute change value\n",
    "                if group_type == 'historic':\n",
    "                    start_year = start_date\n",
    "                    end_year = end_date\n",
    "                else:\n",
    "                    start_year = pd.to_datetime(start_date).year\n",
    "                    end_year = pd.to_datetime(end_date).year\n",
    "                \n",
    "                sum_diff = diff.sum().values\n",
    "                count_diff = diff.count().values\n",
    "                mean_diff = sum_diff/count_diff #diff.mean(skipna=True).values   \n",
    "                \n",
    "                # Save values\n",
    "                Indexes.append(index)\n",
    "                Counts.append(list(h.compute()))\n",
    "                Bins.append(list(bins))\n",
    "                Sum.append(sum_diff)\n",
    "                Count.append(count_diff)\n",
    "                Mean.append(mean_diff)\n",
    "                Depth.append(depth)\n",
    "                Dates.append([start_year, end_year])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    df_change = pd.DataFrame({\"index\": Indexes, \"counts\": Counts, \"bins\": Bins, \"sum_diff\":Sum, \"count_diff\":Count, \"mean_diff\":Mean, \"depth\": Depth, \"years\": Dates})\n",
    "                  \n",
    "    return pd.merge(df.drop(columns='geometry'), \n",
    "                    df_change, \n",
    "                    how='left', \n",
    "                    on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precalculate_change_all_years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_change_all_years(df, xds, name, file_path, nBinds=40, bindsRange=[-50, 50]):\n",
    "    indexes = xds[name].attrs.get('id_ints').astype(np.float32)\n",
    "    times = xds.coords.get('time').values\n",
    "    depths = xds.coords.get('depth').values\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n",
    "    \n",
    "    store = pd.HDFStore(file_path)\n",
    "    \n",
    "    for index in tqdm(indexes):\n",
    "        xmin, ymax, xmax, ymin = df.iloc[int(index)]['geometry'].bounds\n",
    "        xds_index = xds.where(xds[name].isin(index)).sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax))\n",
    "        for depth in depths:\n",
    "            for i in range(len(times)):\n",
    "                for j in range(len(times)-1-i):\n",
    "                    try:\n",
    "                        start_date = times[i]\n",
    "                        end_date = times[i+j+1]\n",
    "                        \n",
    "                        # Get difference between two dates\n",
    "                        diff = xds_index.loc[dict(time=end_date, depth=depth)] - xds_index.loc[dict(time=start_date, depth=depth)]\n",
    "                        \n",
    "                        # Get counts and binds of the histogram\n",
    "                        h, bins = da.histogram(diff.stocks, bins=nBinds, range=bindsRange)\n",
    "                        \n",
    "                        # Compute change value\n",
    "                        start_year = pd.to_datetime(start_date).year\n",
    "                        end_year = pd.to_datetime(end_date).year\n",
    "                        mean_diff = diff.stocks.mean(skipna=True).values                  \n",
    "        \n",
    "                        df_row = pd.DataFrame({\"index\": int(index), \"counts\": str(h.compute()), \"bins\": str(bins), \"mean_diff\":mean_diff, \"depth\": depth, \"years\": str([start_year, end_year])}, index=[int(index)])\n",
    "    \n",
    "                        df_row.to_hdf(file_path, 'table', append=True, min_itemsize={\"counts\": 500,\"bins\":500, \"years\":100})\n",
    "        \n",
    "                    except:\n",
    "                        pass\n",
    "                          \n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precalculate_time_series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_time_series(df, xds, name, variable, group):\n",
    "    indexes = xds[name].attrs.get('id_ints').astype(np.float32)\n",
    "    depths = xds.coords.get('depth').values\n",
    "\n",
    "    if group == 'historic':\n",
    "        years = xds.coords.get('time').values\n",
    "    else:\n",
    "        years = list(pd.DatetimeIndex(xds.coords.get('time').values).year)\n",
    "    \n",
    "    Indexes = []\n",
    "    Value = []\n",
    "    Depth = []\n",
    "    Years = []\n",
    "    \n",
    "    for index in tqdm(indexes):\n",
    "        xmin, ymax, xmax, ymin = df.iloc[int(index)]['geometry'].bounds\n",
    "        xds_index = xds.where(xds[name].isin(index)).sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax))\n",
    "        for depth in depths:\n",
    "            try:\n",
    "                if variable == 'concentration':\n",
    "                    xds_var = xds_index[variable]/10.\n",
    "                else:\n",
    "                    xds_var = xds_index[variable]\n",
    "                    \n",
    "                # Get mean values\n",
    "                value = xds_var.sel(depth=depth).mean(['lon', 'lat']).values\n",
    "               \n",
    "                # Save values\n",
    "                Indexes.append(int(index))\n",
    "                Value.append(value)\n",
    "                Depth.append(depth)  \n",
    "                Years.append(years)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    df_mean = pd.DataFrame({\"index\": Indexes, \"mean_values\":Value, \"depth\": Depth, 'years': Years})\n",
    "    \n",
    "    return pd.merge(df.drop(columns='geometry'), \n",
    "                    df_mean, \n",
    "                    how='left', \n",
    "                    on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precalculate_time_series_by_geom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_time_series_by_geom(df, xds, name, variable, group):\n",
    "    indexes = list(df['index'])\n",
    "    depths = xds.coords.get('depth').values\n",
    "\n",
    "    if group == 'historic':\n",
    "        years = xds.coords.get('time').values\n",
    "    else:\n",
    "        years = list(pd.DatetimeIndex(xds.coords.get('time').values).year)\n",
    "    \n",
    "    Indexes = []\n",
    "    Sum = []\n",
    "    Count = []\n",
    "    Value = []\n",
    "    Depth = []\n",
    "    Years = []\n",
    "    \n",
    "    for index in indexes:\n",
    "        xmin, ymax, xmax, ymin = df.iloc[index]['geometry'].bounds\n",
    "        xds_index = xds.sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax)).copy()\n",
    "        try:\n",
    "            # Rasterize geometry\n",
    "            ds_mask = create_ds_mask_by_geom(df.iloc[index:index+1], xds_index, name='mask', lon_name='lon', lat_name='lat')\n",
    "            xds_index['mask'] = ds_mask\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for depth in depths:\n",
    "            try:\n",
    "                if variable == 'concentration':\n",
    "                    xds_var = xds_index.where(xds_index['mask'].isin(index)).sel(depth=depth)[variable]/10.\n",
    "                else:\n",
    "                    xds_var = xds_index.where(xds_index['mask'].isin(index)).sel(depth=depth)[variable]\n",
    "                    \n",
    "                # Get mean values\n",
    "                sums = xds_var.sum(['lon', 'lat']).values\n",
    "                counts = xds_var.count(['lon', 'lat']).values\n",
    "                values = sums/counts\n",
    "               \n",
    "                # Save values\n",
    "                Indexes.append(int(index))\n",
    "                Sum.append(list(sums))\n",
    "                Count.append(list(counts))\n",
    "                Value.append(list(values))\n",
    "                Depth.append(depth)  \n",
    "                Years.append(years)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    df_mean = pd.DataFrame({\"index\": Indexes, \"sum_values\":Sum, \"count_values\":Count, \"mean_values\":Value, \"depth\": Depth, 'years': Years})\n",
    "    \n",
    "    return pd.merge(df.drop(columns='geometry'), \n",
    "                    df_mean, \n",
    "                    how='left', \n",
    "                    on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save_precalculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_precalculations(vector_data, variable = 'stocks', group_type='experimental_dataset', output_type = 'change', root_path = '../data/precalculations/'):\n",
    "    final_table_names = {'biomes': ['biomes_0', 'biomes_1'], \n",
    "     'political_boundaries': ['political_boundaries_0', 'political_boundaries_1'],\n",
    "     'landforms': ['landforms_0', 'landforms_1_Island', 'landforms_1_Coast', 'landforms_1_Range/mtn', 'landforms_1_Pen/cape', 'landforms_1_Desert', 'landforms_1_Plateau', 'landforms_1_Geoarea', 'landforms_1_Plain', 'landforms_1_Depression', 'landforms_1_Valley', 'landforms_1_Wetlands', 'landforms_1_Delta', 'landforms_1_Basin', 'landforms_1_Lowland', 'landforms_1_Gorge', 'landforms_1_Tundra', 'landforms_1_Isthmus', 'landforms_1_Foothills', 'landforms_1_Peninsula'],\n",
    "     'hydrological_basins': ['hydrological_basins_0', 'hydrological_basins_1']}\n",
    "    \n",
    "    for name in final_table_names.keys():\n",
    "        for n, old_name in enumerate(final_table_names[name]):\n",
    "            if n == 0:\n",
    "                df = pd.DataFrame(columns=vector_data[old_name].columns)\n",
    "            if old_name in vector_data.keys():\n",
    "                df = pd.concat([df, vector_data[old_name]])\n",
    "                \n",
    "        df.drop(columns='index', inplace=True)  \n",
    "        df['variable'] = variable\n",
    "        df['group_type'] = group_type\n",
    "        \n",
    "        df.to_csv(root_path+name+'_'+group_type+'_'+variable+'_'+output_type+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save_precalculations_level1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_precalculations_level1(vector_data, variable = 'stocks', group_type='experimental_dataset', output_type = 'change', root_path = '../data/precalculations/'):\n",
    "    names = ['political_boundaries_1', 'biomes_1', 'hydrological_basins_1', 'landforms_1']\n",
    "\n",
    "    for name in names:\n",
    "        df = vector_data[name].copy()\n",
    "        df.drop(columns='index', inplace=True)  \n",
    "        df['variable'] = variable\n",
    "        df['group_type'] = group_type\n",
    "        \n",
    "        df.to_csv(root_path+name+'_'+group_type+'_'+variable+'_'+output_type+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compute_level_0_change**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_level_0_change(geo_name, group_type, variable, vector_data_0):\n",
    "    df = pd.read_csv(f'../data/precalculations/{geo_name}_1_{group_type}_{variable}_change.csv')\n",
    "    df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    df = df[df['depth'] != 'stocks']\n",
    "    df = df[df['id'].notna()]\n",
    "    df = df.astype({'id': int, 'id_0': int, 'sum_diff':'float64', 'count_diff':'float64', 'mean_diff':'float64'})\n",
    "    \n",
    "    for n, depth in enumerate(df['depth'].unique()):\n",
    "        df_tmp = df[df['depth'] == depth].copy()\n",
    "        \n",
    "        if not df_tmp.empty:\n",
    "            df_tmp['counts'] = df_tmp['counts'].apply(lambda x: np.array(json.loads(x)))\n",
    "            df_counts = df_tmp[['id_0', 'counts']].groupby('id_0').sum().reset_index()\n",
    "            df_counts['bins'] = df_tmp['bins'].iloc[0]\n",
    "            \n",
    "            df_diff = df_tmp[['id_0', 'sum_diff', 'count_diff']].groupby('id_0').sum().reset_index()\n",
    "            df_diff['mean_diff'] = df_diff['sum_diff']/df_diff['count_diff']\n",
    "            \n",
    "            df_change_depth = pd.merge(df_counts, df_diff, on='id_0', how='left')\n",
    "            \n",
    "            df_change_depth['depth'] = depth\n",
    "            for column in ['years', 'variable', 'group_type']:\n",
    "                df_change_depth[column] = df_tmp[column].iloc[0]\n",
    "                \n",
    "            if n == 0:\n",
    "                df_change = df_change_depth\n",
    "            else: \n",
    "                df_change = pd.concat([df_change, df_change_depth])\n",
    "     \n",
    "    df_change = pd.merge(vector_data_0[f'{geo_name}_0'].drop(columns='geometry').astype({'id_0': int}), df_change.astype({'id_0': int}), on='id_0', how='left')\n",
    "    \n",
    "    return pd.concat([df_change.sort_values('id'), df.sort_values('id')]).drop(columns='index').reset_index().drop(columns='index').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compute_level_0_time_series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_level_0_time_series(geo_name, group_type, variable, vector_data_0):\n",
    "    df = pd.read_csv(f'../data/precalculations/{geo_name}_1_{group_type}_{variable}_time_series.csv')\n",
    "    df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    df = df[df['depth'] != 'stocks']\n",
    "    df = df[df['id'].notna()]\n",
    "    df = df.astype({'id': int, 'id_0': int})\n",
    "    \n",
    "    for n, depth in enumerate(df['depth'].unique()):\n",
    "        df_tmp = df[df['depth'] == depth].copy()\n",
    "        \n",
    "        if not df_tmp.empty:\n",
    "            df_tmp['sum_values'] = df_tmp['sum_values'].apply(lambda x: np.array(json.loads(x)))\n",
    "            df_tmp['count_values'] = df_tmp['count_values'].apply(lambda x: np.array(json.loads(x)))\n",
    "            \n",
    "            df_time_series_depth = df_tmp[['id_0', 'sum_values', 'count_values']].groupby('id_0').sum().reset_index()\n",
    "            df_time_series_depth['mean_values'] = df_time_series_depth['sum_values']/df_time_series_depth['count_values']\n",
    "            \n",
    "            df_time_series_depth['depth'] = depth\n",
    "            for column in ['years', 'variable', 'group_type']:\n",
    "                df_time_series_depth[column] = df_tmp[column].iloc[0]\n",
    "                \n",
    "            if n == 0:\n",
    "                df_time_series = df_time_series_depth\n",
    "            else: \n",
    "                df_time_series = pd.concat([df_time_series, df_time_series_depth])\n",
    "                \n",
    "    df_time_series = pd.merge(vector_data_0[f'{geo_name}_0'].drop(columns='geometry').astype({'id_0': int}), df_time_series.astype({'id_0': int}).astype({'id_0': int}), on='id_0', how='left')\n",
    "    \n",
    "    return pd.concat([df_time_series.sort_values('id'), df.sort_values('id')]).drop(columns='index').reset_index().drop(columns='index').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `xarray.Dataset` from `Zarr` in Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3://soils-revealed/global-dataset.zarr' #'s3://soils-revealed/experimental-dataset.zarr' \n",
    "group = 'historic'#'concentration' #stocks\n",
    "ds = read_dataset_from_zarr(s3_path, group)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From local folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/rewilding.zarr' #'../data/experimental-dataset.zarr' \n",
    "group = 'future' #'recent' #'concentration' #stocks\n",
    "ds = read_dataset_from_zarr_local(path, group)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data = prepare_vector_data_political_boundaries(iso='VNM', tolerance=0.075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the data mask by rasterizing the vector data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = list(vector_data.keys())[1:2]\n",
    "for name in names:\n",
    "    print(f'Create the data mask for {name}:')\n",
    "    da_mask = create_ds_mask(vector_data[name], ds, name, lon_name='lon', lat_name='lat')\n",
    "    \n",
    "    ds[name] = da_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "#### **Example**\n",
    "Select subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eg = ds.isel(time=[0,4]).copy()\n",
    "times = ds_eg.coords.get('time').values\n",
    "depths = ds_eg.coords.get('depth').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vector_data['political_boundaries_0'].copy()\n",
    "index = 0\n",
    "xmin, ymax, xmax, ymin = df.iloc[index]['geometry'].bounds\n",
    "ds_eg = ds_eg.sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax)).copy()\n",
    "ds_mask = create_ds_mask(df.iloc[index:index+1], ds_eg, name='mask', lon_name='lon', lat_name='lat')\n",
    "ds_eg['mask'] = ds_mask\n",
    "ds_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = ds_eg.where(ds_eg['mask'].isin(index)).loc[dict(time=times[1], depth=depths[0])] - ds_eg.loc[dict(time=times[0], depth=depths[0])]\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(diff.stocks.values, cmap='RdYlBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with `xhistogram`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-5, 5,11)\n",
    "h = histogram(diff.stocks, bins=[bins], dim=['lat', 'lon'])\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    h.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot change distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    count = h.values\n",
    "    \n",
    "x_min = bins[:-1]\n",
    "plot_hist(x_min, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with `da.histogram`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, bins = da.histogram(diff.stocks, bins=60, range=[-30, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot change distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    count = h.compute()\n",
    "\n",
    "x_min = bins[:-1]\n",
    "plot_hist(x_min, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display change value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.to_datetime(times[1]).year - pd.to_datetime(times[0]).year\n",
    "mean_diff = diff.stocks.mean(skipna=True).compute().values\n",
    "change = mean_diff/years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Soil Organic Carbon Stock Change: {change} t C/ha year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entire` DataFrame`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vector_data_new = {}\n",
    "for name in list(vector_data.keys()):\n",
    "    print(f'Precalculating change for {name}:')\n",
    "    df = vector_data[name].copy()\n",
    "    vector_data_new[name] = precalculate_change(df, ds, name, variable='stocks', nBinds=80, bindsRange=[-50, 50])\n",
    "\n",
    "save_precalculations(vector_data_new, variable = 'stocks', group_type='experimental_dataset', output_type = 'change', root_path = '../data/precalculations/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot change distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vector_data_new['political_boundaries_1'].copy()\n",
    "index = 10\n",
    "count = df['counts'].iloc[index].copy()\n",
    "bins = df['bins'].iloc[index].copy()\n",
    "mean_diff = df['mean_diff'].iloc[index].copy()\n",
    "years = df['years'].iloc[index].copy()\n",
    "x_min = bins[:-1]\n",
    "per = count/count.sum()*100\n",
    "print(f'Soil Organic Carbon Stock Change: {mean_diff/(years[1]-years[0])} t C/ha year')\n",
    "plot_hist(x_min, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean value\n",
    "#### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eg = ds.copy()\n",
    "df = vector_data['political_boundaries_0'].copy()\n",
    "index = 0\n",
    "xmin, ymax, xmax, ymin = df.iloc[index]['geometry'].bounds\n",
    "ds_eg = ds_eg.sel(lon=slice(xmin, xmax), lat=slice(ymin, ymax)).copy()\n",
    "ds_mask = create_ds_mask(df.iloc[index:index+1], ds_eg, name='mask', lon_name='lon', lat_name='lat')\n",
    "ds_eg['mask'] = ds_mask\n",
    "ds_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eg.stocks.where(ds_eg['mask'] == index).mean(['lon', 'lat']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eg.stocks.where(ds_eg['mask'] == index).mean(['lon', 'lat']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_eg.stocks\n",
    ".isel(time=slice(0,12))\n",
    " .where(ds_eg['mask'] == index)\n",
    " .plot.imshow(col='time', col_wrap=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entire` DataFrame`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vector_data_new = {}\n",
    "for name in list(vector_data.keys()):\n",
    "    print(f'Precalculating mean values for {name}:')\n",
    "    df = vector_data[name].copy()\n",
    "    vector_data_new[name] = precalculate_time_series(df, ds, name, variable='stocks')\n",
    "    \n",
    "save_precalculations(vector_data_new, variable = 'stocks', group_type='experimental_dataset', output_type = 'time_series', root_path = '../data/precalculations/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vector_data_new['political_boundaries_1'].copy()\n",
    "index = 10\n",
    "mean_values = df['mean_values'].iloc[index].copy()\n",
    "years = df['years'].iloc[index].copy()\n",
    "plt.plot(years, mean_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `groupby`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grouped_xds = ds.groupby(ds['landforms_Plateau'])\n",
    "grid_mean = grouped_xds.mean().rename({\"stocks\": \"mean\"})\n",
    "grid_mean.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## All the computational process at once\n",
    "**Input variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_path = 's3://soils-revealed/global-dataset.zarr' #'s3://soils-revealed/experimental-dataset.zarr' \n",
    "path = '../data/experimental-dataset.zarr'#'../data/global-dataset.zarr' \n",
    "group = 'stocks' #'concentration' #'historic' #'recent'  \n",
    "variable = 'stocks' #'concentration' #'stocks'  \n",
    "iso = 'ARG'\n",
    "tolerance = 0.075\n",
    "group_type = 'experimental_dataset'  # 'historic' #'recent'\n",
    "nBinds = [80] #[80] #[40, 40, 60] #[10]  \n",
    "bindsRange = [[-50, 50]] #[[-10, 10]] #[[-20,20], [-40,40], [-60,60]] #[[-5,5]] \n",
    "level=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation of level 1 geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read xarray.Dataset from Zarr\n",
    "print('Reading  xarray.Dataset.')\n",
    "ds = read_dataset_from_zarr_local(path, group)\n",
    "\n",
    "# Read vector data\n",
    "print('Reading  vector data.')\n",
    "vector_data = prepare_vector_data_level(iso=iso, tolerance=tolerance, level=level)\n",
    "\n",
    "# Change bboxes\n",
    "if level == 1 and iso == None:\n",
    "    # Alaska\n",
    "    vector_data['political_boundaries_1'].at[1707,'bbox'] = '[-179.1506, 51.2097, -125, 72.6875]'\n",
    "    \n",
    "names = vector_data.keys()\n",
    "# Precalculate change distribution\n",
    "print('Precalculating change distribution.')\n",
    "vector_data_new = {}\n",
    "for name in names:\n",
    "    print(f'Precalculating change for {name}:')\n",
    "    vector_data_new[name] = precalculate_change_by_geom(vector_data[name], ds, name, variable=variable, group_type=group_type, nBinds=nBinds, bindsRange=bindsRange)\n",
    "\n",
    "save_precalculations_level1(vector_data_new, variable = variable, group_type=group_type, output_type = 'change', root_path = '../data/precalculations/')\n",
    "\n",
    "## Precalculate time series\n",
    "print('Precalculating time series.')\n",
    "vector_data_new = {}\n",
    "for name in names:\n",
    "    print(f'Precalculating mean values for {name}:')\n",
    "    vector_data_new[name] = precalculate_time_series_by_geom(vector_data[name], ds, name, variable=variable, group=group)\n",
    "    \n",
    "save_precalculations_level1(vector_data_new, variable = variable, group_type=group_type, output_type = 'time_series', root_path = '../data/precalculations/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation of level 0 geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data_0 = prepare_vector_data_level(iso=iso, tolerance=tolerance, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for geom_name in ['political_boundaries', 'landforms', 'biomes', 'hydrological_basins']:\n",
    "    df_out = compute_level_0_change(geom_name, group_type, variable, vector_data_0)\n",
    "    df_out.to_csv(f'../data/precalculations/{geom_name}_{group_type}_{variable}_change.csv', index=False)\n",
    "    df_out = compute_level_0_time_series(geom_name, group_type, variable, vector_data_0)\n",
    "    df_out.to_csv(f'../data/precalculations/{geom_name}_{group_type}_{variable}_time_series.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_types = {#'crop': ['crop_I', 'crop_MG', 'crop_MGI'],\n",
    "              'grass': ['grass_full', 'grass_part'],\n",
    "              'degradation': ['degradation_ForestToCrop', 'degradation_ForestToGrass', 'degradation_NoDeforestation'],\n",
    "              'rewilding': ['rewilding']}\n",
    "\n",
    "group = 'future' \n",
    "variable = 'stocks' \n",
    "iso = None\n",
    "tolerance = 0.075\n",
    "level=1\n",
    "binds = {'crop': [30],\n",
    "        'grass': [30],\n",
    "        'degradation': [50],\n",
    "        'rewilding': [60]}\n",
    "ranges = {'crop': [[0,30]],\n",
    "         'grass': [[0,30]],\n",
    "         'degradation': [[-50,1]],\n",
    "         'rewilding': [[-30,30]]}\n",
    "\n",
    "# Read vector data\n",
    "print('Reading  vector data.')\n",
    "vector_data = prepare_vector_data_level(iso=iso, tolerance=tolerance, level=level)\n",
    "vector_data_0 = prepare_vector_data_level(iso=iso, tolerance=tolerance, level=0)\n",
    "\n",
    "# Change bboxes\n",
    "if level == 1 and iso == None:\n",
    "    # Alaska\n",
    "    vector_data['political_boundaries_1'].at[1707,'bbox'] = '[-179.1506, 51.2097, -125, 72.6875]'\n",
    "    \n",
    "for group_key in group_types.keys():\n",
    "    nBinds = binds[group_key]\n",
    "    bindsRange = ranges[group_key]\n",
    "    for group_type in group_types[group_key]:\n",
    "        print(f'Computing values for {group_type}!')\n",
    "        path = f'../data/{group_type}.zarr'\n",
    "\n",
    "        # Read xarray.Dataset from Zarr\n",
    "        print('Reading  xarray.Dataset.')\n",
    "        ds = read_dataset_from_zarr_local(path, group)\n",
    "        \n",
    "        names = vector_data.keys()\n",
    "        # Precalculate change distribution\n",
    "        print('Precalculating change distribution.')\n",
    "        vector_data_new = {}\n",
    "        for name in names:\n",
    "            print(f'Precalculating change for {name}:')\n",
    "            vector_data_new[name] = precalculate_change_by_geom(vector_data[name], ds, name, variable=variable, group_type=group_type, nBinds=nBinds, bindsRange=bindsRange)\n",
    "        \n",
    "        save_precalculations_level1(vector_data_new, variable = variable, group_type=group_type, output_type = 'change', root_path = '../data/precalculations/')\n",
    "        \n",
    "        ## Precalculate time series\n",
    "        print('Precalculating time series.')\n",
    "        vector_data_new = {}\n",
    "        for name in names:\n",
    "            print(f'Precalculating mean values for {name}:')\n",
    "            vector_data_new[name] = precalculate_time_series_by_geom(vector_data[name], ds, name, variable=variable, group=group)\n",
    "            \n",
    "        save_precalculations_level1(vector_data_new, variable = variable, group_type=group_type, output_type = 'time_series', root_path = '../data/precalculations/')\n",
    "        \n",
    "        for geom_name in ['political_boundaries', 'landforms', 'biomes', 'hydrological_basins']:\n",
    "            df_out = compute_level_0_change(geom_name, group_type, variable, vector_data_0)\n",
    "            df_out.to_csv(f'../data/precalculations/{geom_name}_{group_type}_{variable}_change.csv', index=False)\n",
    "            df_out = compute_level_0_time_series(geom_name, group_type, variable, vector_data_0)\n",
    "            df_out.to_csv(f'../data/precalculations/{geom_name}_{group_type}_{variable}_time_series.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Political boundaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Landforms\n",
      "Reading Biomes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "/opt/conda/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hydrological basins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersecting areas with the selected country\n",
      "landforms_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/geopandas/geodataframe.py:831: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super(GeoDataFrame, self).__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomes_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/geopandas/geodataframe.py:831: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super(GeoDataFrame, self).__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrological_basins_1\n"
     ]
    }
   ],
   "source": [
    "vector_data = prepare_vector_data_level(iso=iso, tolerance=tolerance, level=level)\n",
    "vector_data_0 = prepare_vector_data_level(iso=iso, tolerance=tolerance, level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Old version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read xarray.Dataset from Zarr\n",
    "print('Reading  xarray.Dataset.')\n",
    "ds = read_dataset_from_zarr_local(path, group)\n",
    "\n",
    "# Read vector data\n",
    "print('Reading  vector data.')\n",
    "vector_data = prepare_vector_data(iso=iso, tolerance=tolerance)\n",
    "\n",
    "# Create the data mask by rasterizing the vector data\n",
    "print('Rasterizing the vector data.')\n",
    "names = list(vector_data.keys())\n",
    "#names.remove('landforms_Dragons-be-here')\n",
    "for name in names:\n",
    "    print(f'Create the data mask for {name}:')\n",
    "    da_mask = create_ds_mask(vector_data[name], ds, name, lon_name='lon', lat_name='lat')\n",
    "    \n",
    "    ds[name] = da_mask\n",
    "    \n",
    "## Precalculate change distribution\n",
    "#print('Precalculating change distribution.')\n",
    "#vector_data_new = {}\n",
    "#for name in names:\n",
    "#    print(f'Precalculating change for {name}:')\n",
    "#    df = vector_data[name].copy()\n",
    "#    vector_data_new[name] = precalculate_change(df, ds, name, variable=variable, group_type=group_type, nBinds=nBinds, bindsRange=bindsRange)\n",
    "#\n",
    "save_precalculations(vector_data_new, variable = variable, group_type=group_type, output_type = 'change', root_path = '../data/precalculations/')\n",
    "\n",
    "## Precalculate time series\n",
    "print('Precalculating time series.')\n",
    "vector_data_new = {}\n",
    "for name in names:\n",
    "    print(f'Precalculating mean values for {name}:')\n",
    "    df = vector_data[name].copy()\n",
    "    vector_data_new[name] = precalculate_time_series(df, ds, name, variable=variable, group=group)\n",
    "    \n",
    "save_precalculations(vector_data_new, variable = variable, group_type=group_type, output_type = 'time_series', root_path = '../data/precalculations/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../data/precalculations/'\n",
    "file_list = os.listdir(dir_path)\n",
    "groups = ['biomes', 'hydrological_basins', 'landforms', 'political_boundaries']\n",
    "subgroups = ['change', 'time_series']\n",
    "group_types = ['experimental_dataset', 'historic', 'recent']\n",
    "\n",
    "for group in groups:\n",
    "    group_list = [s for s in file_list if group in s]\n",
    "    group_list = [s for s in group_list if not ('_1_' in s)]\n",
    "    group_types = ['experimental_dataset', 'historic', 'recent']\n",
    "    new_file_list = []\n",
    "    for group_type in group_types:\n",
    "        new_file_list.append([s for s in group_list if group_type in s])\n",
    "        \n",
    "    group_list = [item for l in new_file_list for item in l]\n",
    "    \n",
    "    for subgroup in subgroups:\n",
    "        sub_list = [s for s in group_list if subgroup in s]\n",
    "        for n, file in enumerate(sub_list):\n",
    "            if n == 0:\n",
    "                df = pd.read_csv(dir_path+file, index_col=[0])\n",
    "                if 'stocks' in file: df.rename(columns={'group': 'group_type'}, inplace=True)\n",
    "            else:\n",
    "                df_new = pd.read_csv(dir_path+file, index_col=[0])\n",
    "                if 'stocks' in file: df_new.rename(columns={'group': 'group_type'}, inplace=True)\n",
    "                df = pd.concat([df, df_new])\n",
    "                \n",
    "        df.to_csv(dir_path+group+'_'+subgroup+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload precalculations to Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carto.auth import APIKeyAuthClient\n",
    "from carto.datasets import DatasetManager\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carto_api_key = getpass.getpass('Carto API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USR_BASE_URL = \"https://35.233.41.65/user/skydipper/\"\n",
    "auth_client = APIKeyAuthClient(api_key=carto_api_key, base_url=USR_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import local datasets into CARTO via the Import API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here the path to a local file or remote URL\n",
    "LOCAL_FILE_OR_URL = \"../data/precalculations/biomes_change.csv\"\n",
    "\n",
    "dataset_manager = DatasetManager(auth_client)\n",
    "dataset = dataset_manager.create(LOCAL_FILE_OR_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM political_boundaries_time_series WHERE variable = 'stocks' AND depth = '0-30' AND group_type =  'experimental_dataset' AND gid_1 = 'ARG.20_1'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlCarto = f\"https://skydipper.35.233.41.65/api/v2/sql\"\n",
    "urlCarto = f\"http://35.233.41.65/user/skydipper/api/v2/sql\"\n",
    "\n",
    "sql = {\"q\": query}\n",
    "r = requests.get(urlCarto, params=sql)\n",
    "\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(data.get(\"rows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
