{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MBTiles for Soils-revealed platform\n",
    "\n",
    "Check this [notebook](https://github.com/Vizzuality/sci_team_data_bank/blob/master/Encyclopedia/map_tile_processing/MBTiles_from_Carto_data.ipynb) for further information on the creation of `MBTiles`.\n",
    "\n",
    "## Table of Contents\n",
    "### [Python libraries](#libraries)\n",
    "### [Utils](#utils)\n",
    "- **[df_from_carto](#df_from_carto)**\n",
    "- **[long_lasting_SQL_queries](#long_lasting_SQL_queries)**\n",
    "- **[create_mbtiles](#create_mbtiles)**\n",
    "\n",
    "### [Read data from different sources ](#read_data)\n",
    "- **[Biomes](#biomes)**\n",
    "- **[World Database on Protected Areas](#protected_areas)**\n",
    "- **[River basins](#river_basins)**\n",
    "- **[Political boundaries](#political_boundaries)**\n",
    "\n",
    "### [Create `MBTiles`](#create_mbtiles_2)\n",
    "### [Show `MBTiles` in our localhost](#show_mbtiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>\n",
    "### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely.wkb \n",
    "from shapely.ops import cascaded_union\n",
    "from carto.auth import APIKeyAuthClient\n",
    "from carto.sql import BatchSQLClient\n",
    "from carto.sql import SQLClient\n",
    "from tqdm import tqdm\n",
    "import getpass\n",
    "import subprocess\n",
    "import time\n",
    "import LMIPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='utils'></a>\n",
    "### Utils\n",
    "<a id='df_from_carto'></a>\n",
    "**df_from_carto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_carto(account, query):\n",
    "    \"\"\"\n",
    "    It gets data by querying a carto table and converts it into a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    urlCarto = f\"https://{account}.carto.com/api/v2/sql\"\n",
    "    \n",
    "    sql = {\"q\": query}\n",
    "    r = requests.get(urlCarto, params=sql)\n",
    "    \n",
    "    data = r.json()\n",
    "    \n",
    "    df = gpd.GeoDataFrame(data.get(\"rows\"))\n",
    "    if 'the_geom' in df.columns:\n",
    "        # Change geometry from WKB to WKT format\n",
    "        df['geometry'] = df.apply(lambda x: shapely.wkb.loads(x['the_geom'],hex=True), axis=1 )\n",
    "        df.drop(columns=['the_geom'], inplace=True)\n",
    "        if 'the_geom_webmercator' in df.columns:\n",
    "            df.drop(columns=['the_geom_webmercator'], inplace=True)\n",
    "        df.crs = {'init': 'epsg:4326'}\n",
    "        df = df.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='long_lasting_SQL_queries'></a>\n",
    "**long_lasting_SQL_queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_lasting_SQL_queries(account, query, api_key):\n",
    "    # For long lasting SQL queries we use the batch SQL API.\n",
    "    table_name = 'job_result'\n",
    "    \n",
    "    base_url = f'https://{account}.carto.com/'\n",
    "    auth_client = APIKeyAuthClient(api_key=api_key, base_url=base_url)\n",
    "    \n",
    "    sql_query =(f'SELECT * INTO {table_name} FROM ({query}) as job')\n",
    "    \n",
    "    LIST_OF_SQL_QUERIES = [sql_query]\n",
    "    \n",
    "    batchSQLClient = BatchSQLClient(auth_client)\n",
    "    createJob = batchSQLClient.create(LIST_OF_SQL_QUERIES)\n",
    "    \n",
    "    # Check the status of a job with the job_id every 10 s\n",
    "    readJob = batchSQLClient.read(createJob['job_id'])\n",
    "    \n",
    "    timeout = time.time() + 60*60  # 1 hour from now\n",
    "    while readJob.get('status') != 'done':\n",
    "        time.sleep(10)\n",
    "        print(readJob.get('status'))\n",
    "        if readJob.get('status') == 'failed':\n",
    "            print('Job failed.')\n",
    "            break\n",
    "        if time.time() > timeout:\n",
    "            readJob = batchSQLClient.read(createJob['job_id'])\n",
    "            # Cancel a job given its job_id\n",
    "            if readJob.get('status') != 'donne':\n",
    "                cancelJob = batchSQLClient.cancel(createJob['job_id'])     \n",
    "                print('Job cancelled after 1 hour running')\n",
    "                break\n",
    "            \n",
    "        readJob = batchSQLClient.read(createJob['job_id'])\n",
    "       \n",
    "    # Read the table\n",
    "    sql = SQLClient(auth_client)\n",
    "    data = sql.send(\"select * from \"+table_name)\n",
    "    \n",
    "    # Drop the table\n",
    "    sql = SQLClient(auth_client)\n",
    "    sql.send(\"DROP TABLE \"+table_name)\n",
    "    \n",
    "    df = gpd.GeoDataFrame(data.get(\"rows\"))\n",
    "    if 'the_geom' in df.columns:\n",
    "        # Change geometry from WKB to WKT format\n",
    "        df['geometry'] = df.apply(lambda x: shapely.wkb.loads(x['the_geom'],hex=True), axis=1 )\n",
    "        df.drop(columns=['the_geom'], inplace=True)\n",
    "        if 'the_geom_webmercator' in df.columns:\n",
    "            df.drop(columns=['the_geom_webmercator'], inplace=True)\n",
    "        df.crs = {'init': 'epsg:4326'}\n",
    "        df = df.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge_geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_geometries(df, column_name):\n",
    "    df_new = pd.DataFrame(columns=list(df.columns))\n",
    "    geom = []\n",
    "    for value in tqdm(df[column_name].unique()):\n",
    "        df_tmp = df[df[column_name] == value].iloc[:1]\n",
    "        geom.append(cascaded_union(df[df[column_name] == value].geometry))\n",
    "        \n",
    "        df_new = pd.concat([df_new, df_tmp])\n",
    "        \n",
    "    df_new.reset_index(inplace=True)\n",
    "    df_new.drop(columns='index', inplace=True)\n",
    "    df_new.drop(columns='geometry', inplace=True)\n",
    "    df_new['geometry'] = geom\n",
    "\n",
    "    return gpd.GeoDataFrame(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_mbtiles'></a>\n",
    "**create_mbtiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mbtiles(source_path, dest_path, layer_name, opts=\"-zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel\"):\n",
    "    \"\"\"\n",
    "    Use tippecanoe to create a MBTILE at dest_path from source_path.\n",
    "    layer_name is used for the name of the layer in the MBTILE.\n",
    "    Regex file path (/*.geojson) is supported for source_path.\n",
    "    \"\"\"\n",
    "    cmd = f\"tippecanoe -o {dest_path} -l {layer_name} {opts} {source_path}\"\n",
    "    print(f\"Processing: {cmd}\")\n",
    "    r = subprocess.call(cmd, shell=True)\n",
    "    if r == 0:\n",
    "        print(\"Task created\")\n",
    "    else:\n",
    "        print(\"Task failed\")\n",
    "    print(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read_data'></a>\n",
    "### Read data from different sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='biomes'></a>\n",
    "#### **[Biomes](https://resourcewatch.org/data/explore/bio042-Ecoregion-by-Biome)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LMIPy.Dataset('ed1544bb-a092-424e-88c2-8d548f4ef94a')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes = gpd.read_file('../data/mbtiles/bio_042_ecoregions_by_biome_1_14/bio_042_ecoregions_by_biome_1_14.shp')\n",
    "biomes.drop(columns='cartodb_id', inplace=True)\n",
    "biomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes.to_file(\"../data/mbtiles/ecoregions_by_biome.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='protected_areas'></a>\n",
    "#### **[World Database on Protected Areas](https://resourcewatch.org/data/explore/bio007-World-Database-on-Protected-Areas_replacement)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LMIPy.Dataset('2442891a-157a-40e6-9092-ee596e6d30ba')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = gpd.read_file('../data/mbtiles/wdpa_protected_areas/wdpa_protected_areas.shp')\n",
    "areas.drop(columns='cartodb_id', inplace=True)\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_filter = areas[(areas['marine'] == '0') & (areas['iucn_cat'] != 'Not Assigned') & (areas['iucn_cat'] != 'Not Applicable') & (areas['iucn_cat'] != 'Not Reported')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas['iucn_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "api_key = getpass.getpass('Carto account api key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT distinct(iucn_cat) FROM wdpa_protected_areas'\n",
    "df = df_from_carto(account, query)\n",
    "iucn_cats = list(df['iucn_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM wdpa_protected_areas LIMIT 1'\n",
    "df = df_from_carto(account, query)\n",
    "coulmns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.GeoDataFrame(columns=coulmns)\n",
    "\n",
    "for iucn_cat in iucn_cats:\n",
    "    query = \"SELECT * FROM wdpa_protected_areas WHERE iucn_cat = 'Ia' OR iucn_cat = 'Ib' OR iucn_cat = 'II' OR iucn_cat = 'III'\"\n",
    "\n",
    "    df = long_lasting_SQL_queries(account, query, api_key)\n",
    "    \n",
    "    data = pd.concat([data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='river_basins'></a>\n",
    "#### **River basins** ([source](http://www.fao.org/nr/water/aquamaps/))\n",
    "**Major hydrological basins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maj_bas</th>\n",
       "      <th>maj_name</th>\n",
       "      <th>maj_area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4050</td>\n",
       "      <td>Arctic Ocean Islands</td>\n",
       "      <td>2166086</td>\n",
       "      <td>POLYGON ((-28.65000 83.43750, -28.67500 83.437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4050</td>\n",
       "      <td>Arctic Ocean Islands</td>\n",
       "      <td>2166086</td>\n",
       "      <td>POLYGON ((-41.15417 83.33750, -41.19583 83.337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4050</td>\n",
       "      <td>Arctic Ocean Islands</td>\n",
       "      <td>2166086</td>\n",
       "      <td>POLYGON ((-41.28750 83.28750, -41.34167 83.287...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4050</td>\n",
       "      <td>Arctic Ocean Islands</td>\n",
       "      <td>2166086</td>\n",
       "      <td>POLYGON ((-39.06667 83.28750, -39.25417 83.287...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4050</td>\n",
       "      <td>Arctic Ocean Islands</td>\n",
       "      <td>2166086</td>\n",
       "      <td>POLYGON ((-43.39583 83.25000, -43.67917 83.250...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   maj_bas              maj_name  maj_area  \\\n",
       "0     4050  Arctic Ocean Islands   2166086   \n",
       "1     4050  Arctic Ocean Islands   2166086   \n",
       "2     4050  Arctic Ocean Islands   2166086   \n",
       "3     4050  Arctic Ocean Islands   2166086   \n",
       "4     4050  Arctic Ocean Islands   2166086   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-28.65000 83.43750, -28.67500 83.437...  \n",
       "1  POLYGON ((-41.15417 83.33750, -41.19583 83.337...  \n",
       "2  POLYGON ((-41.28750 83.28750, -41.34167 83.287...  \n",
       "3  POLYGON ((-39.06667 83.28750, -39.25417 83.287...  \n",
       "4  POLYGON ((-43.39583 83.25000, -43.67917 83.250...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major = gpd.read_file('../data/mbtiles/Major_hydrological_basins/major_hydrobasins.shp')\n",
    "major.columns = map(str.lower, major.columns)\n",
    "major.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/231 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "major = merge_geometries(major, 'maj_bas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major.to_file(\"../data/mbtiles/major_hydrological_basins.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minor hydrological basins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-rw\"\n",
    "api_key = getpass.getpass('Carto account api key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT maj_bas, maj_name, maj_area, sub_bas, sub_name, sub_area, the_geom FROM hydrobasins_fao_fiona_merged_v01\"\n",
    "\n",
    "minor = long_lasting_SQL_queries(account, query, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor = merge_geometries(minor, 'sub_bas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor.to_file(\"../data/mbtiles/minor_hydrological_basins.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append `GeoDataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major['level'] = 0\n",
    "minor['level'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = major.append(minor, sort=False)\n",
    "data = data[['maj_bas', 'maj_name', 'maj_area', 'sub_bas', 'sub_name', 'sub_area',\n",
    "       'level', 'geometry']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/hydrological_basins.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `Shapefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/hydrological_basins/hydrological_basins.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='political_boundaries'></a>\n",
    "#### **Political boundaries ([source](https://gadm.org/data.html))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gadm36 political boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "api_key = getpass.getpass('Carto account api key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "\n",
    "query = \"SELECT name_0, name_1, name_2, area, size, level, gid_0, gid_1, gid_2, the_geom FROM gadm36_political_boundaries\"\n",
    "\n",
    "data = long_lasting_SQL_queries(account, query, api_key)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36 = gpd.read_file('../data/mbtiles/gadm36_political_boundaries/gadm36_political_boundaries.shp')\n",
    "gadm36.drop(columns='cartodb_id', inplace=True)\n",
    "gadm36.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36 = gadm36[gadm36['level'].isin([0,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36['id'] = np.arange(len(gadm36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36.to_file(\"../data/mbtiles/gadm36_political_boundaries.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disputed boundaries 2018**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "\n",
    "query = \"SELECT gid_0, name_0, name, note, the_geom FROM disputed_boundaries_2018 WHERE gid_0 in ('PAK', 'IND', 'CHN') AND name in ('Indian claim', 'Pakistani claim', 'Chinese claim')\"\n",
    "\n",
    "data = df_from_carto(account, query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Physical Features ([source](file:///Users/ikersanchez/Vizzuality/GitHub/sci_team_data_bank/Projects/soils-revealed/data/mbtiles/ne_10m_geography_regions_elevation_points/ne_10m_geography_regions_elevation_points.README.html))\n",
    "**Physical areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = gpd.read_file('../data/mbtiles/ne_10m_geography_regions_polys/ne_10m_geography_regions_polys.shp')\n",
    "polys = polys[~polys.featurecla.isin(['Island group', 'Dragons-be-here', 'Lake'])]\n",
    "polys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys['featurecla'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys.to_file(\"../data/mbtiles/ne_10m_geography_regions.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_mbtiles_2'></a>\n",
    "### Create `MBTiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {'Hydrological basins': 'hydrological_basins.json',\n",
    "         'Ecoregions': 'ecoregions_by_biome.json',\n",
    "         'Political boundaries': 'gadm36_political_boundaries.json',\n",
    "         'Physical geography regions': 'ne_10m_geography_regions.json'}\n",
    "\n",
    "layers = {'Political boundaries': 'gadm36_political_boundaries.json',\n",
    "         'Hydrological basins': 'hydrological_basins.json'}\n",
    "\n",
    "for layer_name, file in layers.items():\n",
    "    print(layer_name)\n",
    "    source_path = \"../data/mbtiles/\"+file\n",
    "    dest_path = \"../data/mbtiles/\"+file.split('.')[0]+\".mbtiles\"\n",
    "    create_mbtiles(source_path, dest_path, layer_name, opts=\"-zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='show_mbtiles'></a>\n",
    "### Show `MBTiles` in our localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mbview --port 9000 ../data/mbtiles/hydrological_basins.mbtiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='serve_mbtiles'></a>\n",
    "### 7. Serve `MBTiles` in our localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm -p 8080:8000 -v /Users/ikersanchez/Vizzuality/GitHub/sci_team_data_bank/Projects/soils-revealed/data/mbtiles:/tilesets  consbio/mbtileserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste in your browser the following:\n",
    "\n",
    "`http://localhost:8080/services/hydrological_basins`\n",
    "\n",
    "And to see the tiles on a map:\n",
    "\n",
    "`http://localhost:8080/services/hydrological_basins/map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
