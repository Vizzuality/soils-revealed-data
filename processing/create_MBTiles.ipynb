{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MBTiles for Soils-revealed platform\n",
    "\n",
    "Check this [notebook](https://github.com/Vizzuality/sci_team_data_bank/blob/master/Encyclopedia/map_tile_processing/MBTiles_from_Carto_data.ipynb) for further information on the creation of `MBTiles`.\n",
    "\n",
    "## Table of Contents\n",
    "### [Python libraries](#libraries)\n",
    "### [Utils](#utils)\n",
    "- **[df_from_carto](#df_from_carto)**\n",
    "- **[long_lasting_SQL_queries](#long_lasting_SQL_queries)**\n",
    "- **[create_mbtiles](#create_mbtiles)**\n",
    "\n",
    "### [Read data from different sources ](#read_data)\n",
    "- **[Biomes](#biomes)**\n",
    "- **[World Database on Protected Areas](#protected_areas)**\n",
    "- **[River basins](#river_basins)**\n",
    "- **[Political boundaries](#political_boundaries)**\n",
    "\n",
    "### [Create `MBTiles`](#create_mbtiles_2)\n",
    "### [Show `MBTiles` in our localhost](#show_mbtiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>\n",
    "### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely.wkb \n",
    "from shapely.ops import cascaded_union\n",
    "from carto.auth import APIKeyAuthClient\n",
    "from carto.sql import BatchSQLClient\n",
    "from carto.sql import SQLClient\n",
    "from tqdm import tqdm\n",
    "import getpass\n",
    "import subprocess\n",
    "import time\n",
    "import LMIPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='utils'></a>\n",
    "### Utils\n",
    "<a id='df_from_carto'></a>\n",
    "**df_from_carto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_carto(account, query):\n",
    "    \"\"\"\n",
    "    It gets data by querying a carto table and converts it into a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    urlCarto = f\"https://{account}.carto.com/api/v2/sql\"\n",
    "    \n",
    "    sql = {\"q\": query}\n",
    "    r = requests.get(urlCarto, params=sql)\n",
    "    \n",
    "    data = r.json()\n",
    "    \n",
    "    df = gpd.GeoDataFrame(data.get(\"rows\"))\n",
    "    if 'the_geom' in df.columns:\n",
    "        # Change geometry from WKB to WKT format\n",
    "        df['geometry'] = df.apply(lambda x: shapely.wkb.loads(x['the_geom'],hex=True), axis=1 )\n",
    "        df.drop(columns=['the_geom'], inplace=True)\n",
    "        if 'the_geom_webmercator' in df.columns:\n",
    "            df.drop(columns=['the_geom_webmercator'], inplace=True)\n",
    "        df.crs = {'init': 'epsg:4326'}\n",
    "        df = df.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='long_lasting_SQL_queries'></a>\n",
    "**long_lasting_SQL_queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_lasting_SQL_queries(account, query, api_key):\n",
    "    # For long lasting SQL queries we use the batch SQL API.\n",
    "    table_name = 'job_result'\n",
    "    \n",
    "    base_url = f'https://{account}.carto.com/'\n",
    "    auth_client = APIKeyAuthClient(api_key=api_key, base_url=base_url)\n",
    "    \n",
    "    sql_query =(f'SELECT * INTO {table_name} FROM ({query}) as job')\n",
    "    \n",
    "    LIST_OF_SQL_QUERIES = [sql_query]\n",
    "    \n",
    "    batchSQLClient = BatchSQLClient(auth_client)\n",
    "    createJob = batchSQLClient.create(LIST_OF_SQL_QUERIES)\n",
    "    \n",
    "    # Check the status of a job with the job_id every 10 s\n",
    "    readJob = batchSQLClient.read(createJob['job_id'])\n",
    "    \n",
    "    timeout = time.time() + 60*60  # 1 hour from now\n",
    "    while readJob.get('status') != 'done':\n",
    "        time.sleep(10)\n",
    "        print(readJob.get('status'))\n",
    "        if readJob.get('status') == 'failed':\n",
    "            print('Job failed.')\n",
    "            break\n",
    "        if time.time() > timeout:\n",
    "            readJob = batchSQLClient.read(createJob['job_id'])\n",
    "            # Cancel a job given its job_id\n",
    "            if readJob.get('status') != 'donne':\n",
    "                cancelJob = batchSQLClient.cancel(createJob['job_id'])     \n",
    "                print('Job cancelled after 1 hour running')\n",
    "                break\n",
    "            \n",
    "        readJob = batchSQLClient.read(createJob['job_id'])\n",
    "       \n",
    "    # Read the table\n",
    "    sql = SQLClient(auth_client)\n",
    "    data = sql.send(\"select * from \"+table_name)\n",
    "    \n",
    "    # Drop the table\n",
    "    sql = SQLClient(auth_client)\n",
    "    sql.send(\"DROP TABLE \"+table_name)\n",
    "    \n",
    "    df = gpd.GeoDataFrame(data.get(\"rows\"))\n",
    "    if 'the_geom' in df.columns:\n",
    "        # Change geometry from WKB to WKT format\n",
    "        df['geometry'] = df.apply(lambda x: shapely.wkb.loads(x['the_geom'],hex=True), axis=1 )\n",
    "        df.drop(columns=['the_geom'], inplace=True)\n",
    "        if 'the_geom_webmercator' in df.columns:\n",
    "            df.drop(columns=['the_geom_webmercator'], inplace=True)\n",
    "        df.crs = {'init': 'epsg:4326'}\n",
    "        df = df.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge_geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_geometries(df, column_name):\n",
    "    df_new = pd.DataFrame(columns=list(df.columns))\n",
    "    geom = []\n",
    "    for value in tqdm(df[column_name].unique()):\n",
    "        df_tmp = df[df[column_name] == value].iloc[:1]\n",
    "        geom.append(cascaded_union(list(df[df[column_name] == value].geometry)))\n",
    "        \n",
    "        df_new = pd.concat([df_new, df_tmp])\n",
    "        \n",
    "    df_new.reset_index(inplace=True)\n",
    "    df_new.drop(columns='index', inplace=True)\n",
    "    df_new.drop(columns='geometry', inplace=True)\n",
    "    df_new['geometry'] = geom\n",
    "\n",
    "    return gpd.GeoDataFrame(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_mbtiles'></a>\n",
    "**create_mbtiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mbtiles(source_path, dest_path, layer_name, opts=\"-zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel\"):\n",
    "    \"\"\"\n",
    "    Use tippecanoe to create a MBTILE at dest_path from source_path.\n",
    "    layer_name is used for the name of the layer in the MBTILE.\n",
    "    Regex file path (/*.geojson) is supported for source_path.\n",
    "    \"\"\"\n",
    "    cmd = f\"tippecanoe -o {dest_path} -l {layer_name} {opts} {source_path}\"\n",
    "    print(f\"Processing: {cmd}\")\n",
    "    r = subprocess.call(cmd, shell=True)\n",
    "    if r == 0:\n",
    "        print(\"Task created\")\n",
    "    else:\n",
    "        print(\"Task failed\")\n",
    "    print(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read_data'></a>\n",
    "### Read data from different sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='biomes'></a>\n",
    "#### **[Biomes](https://resourcewatch.org/data/explore/bio042-Ecoregion-by-Biome)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LMIPy.Dataset('ed1544bb-a092-424e-88c2-8d548f4ef94a')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes = gpd.read_file('../data/mbtiles/bio_042_ecoregions_by_biome_1_14/bio_042_ecoregions_by_biome_1_14.shp')\n",
    "biomes.drop(columns='cartodb_id', inplace=True)\n",
    "biomes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes_0 = merge_geometries(biomes, 'biome_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty(len(biomes_0))\n",
    "a[:] = np.nan\n",
    "biomes_0['eco_name'] = a\n",
    "biomes_0['realm'] = a\n",
    "biomes_0['eco_biome_'] = a\n",
    "biomes_0['eco_id'] = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append `GeoDataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes_0['level'] = 0\n",
    "biomes['level'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = biomes_0.append(biomes, sort=False)\n",
    "data = data[['biome_name', 'biome_num', 'eco_name', 'realm', 'eco_biome_', 'eco_id',\n",
    "       'level', 'geometry']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add parent ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomes = gpd.read_file('../data/mbtiles/ecoregions_by_biome/ecoregions_by_biome.shp')\n",
    "\n",
    "biomes = pd.merge(biomes, biomes[biomes['level']==0][['biome_name', 'id']], on='biome_name', how='left').rename(columns={'id_x': 'id', 'id_y': 'id_0'})\n",
    "columns = list(biomes.columns)\n",
    "columns.insert(len(columns)-3, columns.pop(len(columns)-1))\n",
    "data = biomes[columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/ecoregions_by_biome.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `Shapefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/ecoregions_by_biome/ecoregions_by_biome.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='protected_areas'></a>\n",
    "#### **[World Database on Protected Areas](https://resourcewatch.org/data/explore/bio007-World-Database-on-Protected-Areas_replacement)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LMIPy.Dataset('2442891a-157a-40e6-9092-ee596e6d30ba')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = gpd.read_file('../data/mbtiles/wdpa_protected_areas/wdpa_protected_areas.shp')\n",
    "areas.drop(columns='cartodb_id', inplace=True)\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_filter = areas[(areas['marine'] == '0') & (areas['iucn_cat'] != 'Not Assigned') & (areas['iucn_cat'] != 'Not Applicable') & (areas['iucn_cat'] != 'Not Reported')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas['iucn_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "api_key = getpass.getpass('Carto account api key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT distinct(iucn_cat) FROM wdpa_protected_areas'\n",
    "df = df_from_carto(account, query)\n",
    "iucn_cats = list(df['iucn_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM wdpa_protected_areas LIMIT 1'\n",
    "df = df_from_carto(account, query)\n",
    "coulmns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.GeoDataFrame(columns=coulmns)\n",
    "\n",
    "for iucn_cat in iucn_cats:\n",
    "    query = \"SELECT * FROM wdpa_protected_areas WHERE iucn_cat = 'Ia' OR iucn_cat = 'Ib' OR iucn_cat = 'II' OR iucn_cat = 'III'\"\n",
    "\n",
    "    df = long_lasting_SQL_queries(account, query, api_key)\n",
    "    \n",
    "    data = pd.concat([data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='river_basins'></a>\n",
    "#### **River basins** ([source](http://www.fao.org/nr/water/aquamaps/))\n",
    "**Major hydrological basins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major = gpd.read_file('../data/mbtiles/Major_hydrological_basins/major_hydrobasins.shp')\n",
    "major.columns = map(str.lower, major.columns)\n",
    "major.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major = merge_geometries(major, 'maj_bas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major.to_file(\"../data/mbtiles/major_hydrological_basins.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minor hydrological basins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-rw\"\n",
    "api_key = getpass.getpass('Carto account api key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT maj_bas, maj_name, maj_area, sub_bas, sub_name, sub_area, the_geom FROM hydrobasins_fao_fiona_merged_v01\"\n",
    "\n",
    "minor = long_lasting_SQL_queries(account, query, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor['name'] = minor['sub_name'] +' ('+ minor['maj_name'] + ')' \n",
    "minor = merge_geometries(minor, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor.drop(columns='name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append `GeoDataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major['level'] = 0\n",
    "minor['level'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major.sort_values(['maj_bas'], inplace=True)\n",
    "# Drop Antarctica\n",
    "major.drop(index=230, inplace=True)\n",
    "minor.sort_values(['maj_bas', 'sub_bas'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = major.append(minor, sort=False)\n",
    "data = data[['maj_bas', 'maj_name', 'maj_area', 'sub_bas', 'sub_name', 'sub_area',\n",
    "       'level', 'geometry']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add parent ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, data[data['level']==0][['maj_name', 'id']], on='maj_name', how='left').rename(columns={'id_x': 'id', 'id_y': 'id_0'})\n",
    "columns = list(data.columns)\n",
    "columns.insert(len(columns)-3, columns.pop(len(columns)-1))\n",
    "data = data[columns]\n",
    "columns = list(data.columns)\n",
    "columns.insert(len(columns)-3, columns.pop(len(columns)-1))\n",
    "data = data[columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add bounding box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bbox'] = data['geometry'].apply(lambda x: str(list(x.bounds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/hydrological_basins.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `Shapefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/hydrological_basins/hydrological_basins.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='political_boundaries'></a>\n",
    "#### **Political boundaries ([source](https://gadm.org/data.html))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gadm36 political boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "api_key = getpass.getpass('Carto account api key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "\n",
    "query = \"SELECT name_0, name_1, name_2, area, size, level, gid_0, gid_1, gid_2, the_geom FROM gadm36_political_boundaries\"\n",
    "\n",
    "data = long_lasting_SQL_queries(account, query, api_key)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36 = gpd.read_file('../data/mbtiles/gadm36_political_boundaries/gadm36_political_boundaries.shp')\n",
    "gadm36.drop(columns='cartodb_id', inplace=True)\n",
    "gadm36.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36 = gadm36[gadm36['level'].isin([0,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm36['id'] = np.arange(len(gadm36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add parent ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file('../data/mbtiles/political_boundaries/political_boundaries.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, data[data['level']==0][['gid_0', 'id']], on='gid_0', how='left').rename(columns={'id_x': 'id', 'id_y': 'id_0'})\n",
    "columns = list(data.columns)\n",
    "columns.insert(len(columns)-3, columns.pop(len(columns)-1))\n",
    "data = data[columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/political_boundaries.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `Shapefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/political_boundaries/political_boundaries.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disputed boundaries 2018**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"wri-01\"\n",
    "\n",
    "query = \"SELECT gid_0, name_0, name, note, the_geom FROM disputed_boundaries_2018 WHERE gid_0 in ('PAK', 'IND', 'CHN') AND name in ('Indian claim', 'Pakistani claim', 'Chinese claim')\"\n",
    "\n",
    "data = df_from_carto(account, query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Physical Features ([source](file:///Users/ikersanchez/Vizzuality/GitHub/sci_team_data_bank/Projects/soils-revealed/data/mbtiles/ne_10m_geography_regions_elevation_points/ne_10m_geography_regions_elevation_points.README.html))\n",
    "**Physical areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = gpd.read_file('../data/mbtiles/ne_10m_geography_regions_polys/ne_10m_geography_regions_polys.shp')\n",
    "polys = polys[~polys.featurecla.isin(['Continent','Island group', 'Dragons-be-here', 'Lake'])]\n",
    "polys.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys_0 = merge_geometries(polys, 'featurecla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty(len(polys_0))\n",
    "a[:] = np.nan\n",
    "for column in list(polys_0.columns)[1:-1]:\n",
    "    polys_0[column] = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append `GeoDataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys_0['level'] = 0\n",
    "polys['level'] = 1\n",
    "\n",
    "data = polys_0.append(polys, sort=False)\n",
    "\n",
    "data = data[['featurecla', 'name', 'region', 'subregion', 'ne_id', 'level', 'geometry']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add bounding box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bbox'] = data['geometry'].apply(lambda x: str(list(x.bounds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add parent ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, data[data['level']==0][['featurecla', 'id']], on='featurecla', how='left').rename(columns={'id_x': 'id', 'id_y': 'id_0'})\n",
    "columns = list(data.columns)\n",
    "columns.insert(len(columns)-3, columns.pop(len(columns)-1))\n",
    "data = data[columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `GeoJSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/ne_10m_geography_regions.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `GeoDataFrame` as `Shapefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_file(\"../data/mbtiles/ne_10m_geography_regions/ne_10m_geography_regions.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['ne_10m_geography_regions', 'ecoregions_by_biome', 'political_boundaries', 'hydrological_basins']\n",
    "datasets = ['ne_10m_geography_regions']\n",
    "for dataset in datasets:\n",
    "    gdf = gpd.read_file(f\"../data/mbtiles/{dataset}/{dataset}.shp\")\n",
    "    gdf['bbox'] = gdf['geometry'].apply(lambda x: str(list(x.bounds)))\n",
    "    gdf.to_file(f\"../data/mbtiles/{dataset}.json\", driver=\"GeoJSON\")\n",
    "    gdf.to_file(f\"../data/mbtiles/{dataset}/{dataset}.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_mbtiles_2'></a>\n",
    "### Create `MBTiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydrological basins\n",
      "Processing: tippecanoe -o ../data/mbtiles/hydrological_basins.mbtiles -l Hydrological basins -zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel ../data/mbtiles/hydrological_basins.json\n",
      "Task created\n",
      "Finished processing\n",
      "Ecoregions\n",
      "Processing: tippecanoe -o ../data/mbtiles/ecoregions_by_biome.mbtiles -l Ecoregions -zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel ../data/mbtiles/ecoregions_by_biome.json\n",
      "Task created\n",
      "Finished processing\n",
      "Political boundaries\n",
      "Processing: tippecanoe -o ../data/mbtiles/political_boundaries.mbtiles -l Political boundaries -zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel ../data/mbtiles/political_boundaries.json\n",
      "Task created\n",
      "Finished processing\n",
      "Physical geography regions\n",
      "Processing: tippecanoe -o ../data/mbtiles/ne_10m_geography_regions.mbtiles -l Physical geography regions -zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel ../data/mbtiles/ne_10m_geography_regions.json\n",
      "Task created\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "layers = {'Hydrological basins': 'hydrological_basins.json',\n",
    "         'Ecoregions': 'ecoregions_by_biome.json',\n",
    "         'Political boundaries': 'political_boundaries.json',\n",
    "         'Physical geography regions': 'ne_10m_geography_regions.json'}\n",
    "\n",
    "for layer_name, file in layers.items():\n",
    "    print(layer_name)\n",
    "    source_path = \"../data/mbtiles/\"+file\n",
    "    dest_path = \"../data/mbtiles/\"+file.split('.')[0]+\".mbtiles\"\n",
    "    create_mbtiles(source_path, dest_path, layer_name, opts=\"-zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='show_mbtiles'></a>\n",
    "### Show `MBTiles` in our localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mbview --port 9000 ../data/mbtiles/ne_10m_geography_regions.mbtiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='serve_mbtiles'></a>\n",
    "### 7. Serve `MBTiles` in our localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm -p 8080:8000 -v /Users/ikersanchez/Vizzuality/GitHub/sci_team_data_bank/Projects/soils-revealed/data/mbtiles:/tilesets  consbio/mbtileserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste in your browser the following:\n",
    "\n",
    "`http://localhost:8080/services/hydrological_basins`\n",
    "\n",
    "And to see the tiles on a map:\n",
    "\n",
    "`http://localhost:8080/services/hydrological_basins/map`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
